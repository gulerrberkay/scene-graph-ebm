2023-02-27 10:01:10,456 maskrcnn_benchmark INFO: Using 1 GPUs
2023-02-27 10:01:10,456 maskrcnn_benchmark INFO: Namespace(config_file='', local_rank=0, skip_test=False, opts=[], distributed=False)
2023-02-27 10:01:10,456 maskrcnn_benchmark INFO: Collecting env info (might take some time)
2023-02-27 10:01:11,599 maskrcnn_benchmark INFO: 
PyTorch version: 1.13.0+cu117
Is debug build: False
CUDA used to build PyTorch: 11.7
ROCM used to build PyTorch: N/A

OS: Fedora Linux 37 (Thirty Seven) (x86_64)
GCC version: (GCC) 12.2.1 20221121 (Red Hat 12.2.1-4)
Clang version: 15.0.7 (Fedora 15.0.7-1.fc37)
CMake version: version 3.26.0-rc3
Libc version: glibc-2.36

Python version: 3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0] (64-bit runtime)
Python platform: Linux-6.1.13-200.fc37.x86_64-x86_64-with-glibc2.36
Is CUDA available: True
CUDA runtime version: Could not collect
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA GeForce GTX 1660 Ti
Nvidia driver version: 525.89.02
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.23.5
[pip3] torch==1.13.0
[pip3] torch-scatter==2.1.0
[pip3] torchaudio==0.13.0
[pip3] torchvision==0.14.0
[conda] Could not collect
        Pillow (9.3.0)
2023-02-27 10:01:11,599 maskrcnn_benchmark INFO: Loaded configuration file configs/e2e_relation_X_101_32_8_FPN_1x.yaml
2023-02-27 10:01:11,600 maskrcnn_benchmark INFO: 
INPUT:
  MIN_SIZE_TRAIN: (600,)
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MAX_SIZE_TEST: 1000
MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d"
  PRETRAINED_DETECTOR_CKPT: "/users/students/r0879687/amager/vg/pretrained-detector/pretrained_faster_rcnn/model_final.pth"
  BACKBONE:
    CONV_BODY: "R-101-FPN" # VGG-16
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    STRIDE_IN_1X1: False
    NUM_GROUPS: 32
    WIDTH_PER_GROUP: 8
  RELATION_ON: True
  ATTRIBUTE_ON: False
  FLIP_AUG: False            # if there is any left-right relation, FLIP AUG should be false
  RPN:
    USE_FPN: True
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)   # from neural-motifs
    PRE_NMS_TOP_N_TRAIN: 6000
    PRE_NMS_TOP_N_TEST: 6000
    POST_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_PER_BATCH: False
    RPN_MID_CHANNEL: 256
  ROI_HEADS:
    USE_FPN: True
    POSITIVE_FRACTION: 0.5
    BG_IOU_THRESHOLD: 0.3
    BATCH_SIZE_PER_IMAGE: 256
    DETECTIONS_PER_IMG: 80
    NMS_FILTER_DUPLICATES: True
  ROI_BOX_HEAD:
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    NUM_CLASSES: 151                # 151 for VG, 1201 for GQA
    MLP_HEAD_DIM: 4096
  ROI_ATTRIBUTE_HEAD:
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    USE_BINARY_LOSS: True           # choose binary, because cross_entropy loss deteriorate the box head, even with 0.1 weight
    POS_WEIGHT: 50.0
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    NUM_ATTRIBUTES: 201             # 201 for VG, 501 for GQA
    MAX_ATTRIBUTES: 10             
    ATTRIBUTE_BGFG_SAMPLE: True    
    ATTRIBUTE_BGFG_RATIO: 3        
  ROI_RELATION_HEAD:
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
    REQUIRE_BOX_OVERLAP: False              # for sgdet, during training, only train pairs with overlap
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True    # for sgdet only, in case some gt boxes are missing
    NUM_CLASSES: 51                 # 51 for VG, 201 for GQA (not contain "to the left of" & "to the right of")
    BATCH_SIZE_PER_IMAGE: 1024      # sample as much as possible
    POSITIVE_FRACTION: 0.25
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_HIDDEN_DIM: 512         #1024 for VCTree
    POOLING_ALL_LEVELS: True
    LABEL_SMOOTHING_LOSS: False
    FEATURE_EXTRACTOR: "RelationFeatureExtractor"
    #################### Select Relationship Model ####################
    #PREDICTOR: "MotifPredictor"
    #PREDICTOR: "VCTreePredictor"
    PREDICTOR: "TransformerPredictor"
    #PREDICTOR: "CausalAnalysisPredictor"
    ################# Parameters for Motif Predictor ##################
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_REL_LAYER: 1
    ############# Parameters for Causal Unbias Predictor ##############
    ### Implementation for paper "Unbiased Scene Graph Generation from Biased Training"
    CAUSAL:
      EFFECT_TYPE: 'none'             # candicates: 'TDE', 'NIE', 'TE', 'none'
      FUSION_TYPE: 'sum'              # candicates: 'sum', 'gate'         
      SEPARATE_SPATIAL: False         # separate spatial in union feature
      CONTEXT_LAYER: "motifs"         # candicates: motifs, vctree, vtranse
      SPATIAL_FOR_VISION: True
      EFFECT_ANALYSIS: True
    ############### Parameters for Transformer Predictor ##############
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      OBJ_LAYER: 4
      REL_LAYER: 2
      NUM_HEAD: 8
      KEY_DIM: 64
      VAL_DIM: 64
      INNER_DIM: 2048
    ############### Parameters for Energy Model ##############
    EBM:
      OBJ_EMBED_DIM: 512
      REL_EMBED_DIM: 512
      OBJ_LABEL_EMBED_DIM: 512
      REL_LABEL_EMBED_DIM: 512
      POOLING_DIM: 512
DATASETS:
  TRAIN: ("VG_stanford_filtered_with_attribute_train",)
  VAL: ("VG_stanford_filtered_with_attribute_val",)
  TEST: ("VG_stanford_filtered_with_attribute_test",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  BIAS_LR_FACTOR: 1
  BASE_LR: 0.0025
  WARMUP_FACTOR: 0.1
  WEIGHT_DECAY: 0.0001
  MOMENTUM: 0.9
  GRAD_NORM_CLIP: 5.0
  STEPS: [0, 30000, 40000 ]
  MAX_ITER: 60000
  VAL_PERIOD: 2000
  CHECKPOINT_PERIOD: 2000
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    # the following paramters are only used for WarmupReduceLROnPlateau
    #TYPE: "WarmupReduceLROnPlateau"    # WarmupMultiStepLR, WarmupReduceLROnPlateau
    TYPE: "WarmupMultiStepLR"
    PATIENCE: 2
    THRESHOLD: 0.001
    COOLDOWN: 0
    FACTOR: 0.05   #0.1
    MAX_DECAY_STEP: 3

  # Equivalent schedules with...
  # 1 GPU:
  #   BASE_LR: 0.0025
  #   MAX_ITER: 60000
  #   STEPS: [0, 30000, 40000]
  # 2 GPUs:
  #   BASE_LR: 0.005
  #   MAX_ITER: 30000
  #   STEPS: [0, 15000, 20000]
  # 4 GPUs:
  #   BASE_LR: 0.01
  #   MAX_ITER: 15000
  #   STEPS: [0, 7500, 10000]
  # 8 GPUs:
  #   BASE_LR: 0.02
  #   MAX_ITER: 7500
  #   STEPS: [0, 3750, 5000]

OUTPUT_DIR: '/users/students/r0879687/amager/vg/output/relation_baseline'
TEST:
  ALLOW_LOAD_FROM_CACHE: False
  RELATION:
    SYNC_GATHER: True      # turn on will slow down the evaluation to solve the sgdet test out of memory problem
    REQUIRE_OVERLAP: False
    LATER_NMS_PREDICTION_THRES: 0.5

2023-02-27 10:01:11,600 maskrcnn_benchmark INFO: Running with config:
AMP_VERBOSE: False
DATALOADER:
  ASPECT_RATIO_GROUPING: True
  NUM_WORKERS: 4
  SIZE_DIVISIBILITY: 32
DATASETS:
  NUM_OBJ_CLASSES: -1
  NUM_REL_CLASSES: -1
  TEST: ('VG_stanford_filtered_with_attribute_test',)
  TRAIN: ('VG_stanford_filtered_with_attribute_train',)
  VAL: ('VG_stanford_filtered_with_attribute_val',)
DTYPE: float32
ENERGY_MODEL:
  DATA_NOISE_VAR: 0.0001
  L2COEFF: 1
  LOSS: ContrastiveDivergence
  META_ARCHITECTURE: GraphEnergyModel
  TEMP: 1
  TRAINIG_MODE: joint
GLOVE_DIR: /users/students/r0879687/amager/vg/glove
INPUT:
  BRIGHTNESS: 0.0
  CONTRAST: 0.0
  FLIP_PROB_TRAIN: 0
  HUE: 0.0
  MAX_SIZE_TEST: 1000
  MAX_SIZE_TRAIN: 1000
  MIN_SIZE_TEST: 600
  MIN_SIZE_TRAIN: (600,)
  PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
  PIXEL_STD: [1.0, 1.0, 1.0]
  SATURATION: 0.0
  TO_BGR255: True
  VERTICAL_FLIP_PROB_TRAIN: 0.0
MODEL:
  ATTRIBUTE_ON: False
  BACKBONE:
    CONV_BODY: R-101-FPN
    FREEZE_CONV_BODY_AT: 2
  BASE_ONLY: True
  CLS_AGNOSTIC_BBOX_REG: False
  DEVICE: cuda
  DEV_RUN: False
  FBNET:
    ARCH: default
    ARCH_DEF: 
    BN_TYPE: bn
    DET_HEAD_BLOCKS: []
    DET_HEAD_LAST_SCALE: 1.0
    DET_HEAD_STRIDE: 0
    DW_CONV_SKIP_BN: True
    DW_CONV_SKIP_RELU: True
    KPTS_HEAD_BLOCKS: []
    KPTS_HEAD_LAST_SCALE: 0.0
    KPTS_HEAD_STRIDE: 0
    MASK_HEAD_BLOCKS: []
    MASK_HEAD_LAST_SCALE: 0.0
    MASK_HEAD_STRIDE: 0
    RPN_BN_TYPE: 
    RPN_HEAD_BLOCKS: 0
    SCALE_FACTOR: 1.0
    WIDTH_DIVISOR: 1
  FLIP_AUG: False
  FPN:
    USE_GN: False
    USE_RELU: False
  GROUP_NORM:
    DIM_PER_GP: -1
    EPSILON: 1e-05
    NUM_GROUPS: 32
  KEYPOINT_ON: False
  MASK_ON: False
  META_ARCHITECTURE: GeneralizedRCNN
  PRETRAINED_DETECTOR_CKPT: /users/students/r0879687/amager/vg/pretrained-detector/pretrained_faster_rcnn/model_final.pth
  RELATION_ON: True
  RESNETS:
    BACKBONE_OUT_CHANNELS: 256
    DEFORMABLE_GROUPS: 1
    NUM_GROUPS: 32
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STAGE_WITH_DCN: (False, False, False, False)
    STEM_FUNC: StemWithFixedBatchNorm
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: False
    TRANS_FUNC: BottleneckWithFixedBatchNorm
    WIDTH_PER_GROUP: 8
    WITH_MODULATED_DCN: False
  RETINANET:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDES: (8, 16, 32, 64, 128)
    ASPECT_RATIOS: (0.5, 1.0, 2.0)
    BBOX_REG_BETA: 0.11
    BBOX_REG_WEIGHT: 4.0
    BG_IOU_THRESHOLD: 0.4
    FG_IOU_THRESHOLD: 0.5
    INFERENCE_TH: 0.05
    LOSS_ALPHA: 0.25
    LOSS_GAMMA: 2.0
    NMS_TH: 0.4
    NUM_CLASSES: 81
    NUM_CONVS: 4
    OCTAVE: 2.0
    PRE_NMS_TOP_N: 1000
    PRIOR_PROB: 0.01
    SCALES_PER_OCTAVE: 3
    STRADDLE_THRESH: 0
    USE_C5: True
  RETINANET_ON: False
  ROI_ATTRIBUTE_HEAD:
    ATTRIBUTE_BGFG_RATIO: 3
    ATTRIBUTE_BGFG_SAMPLE: True
    ATTRIBUTE_LOSS_WEIGHT: 1.0
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MAX_ATTRIBUTES: 10
    NUM_ATTRIBUTES: 201
    POS_WEIGHT: 50.0
    PREDICTOR: FPNPredictor
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_BINARY_LOSS: True
  ROI_BOX_HEAD:
    CONV_HEAD_DIM: 256
    DILATION: 1
    FEATURE_EXTRACTOR: FPN2MLPFeatureExtractor
    MLP_HEAD_DIM: 4096
    NUM_CLASSES: 151
    NUM_STACKED_CONVS: 4
    POOLER_RESOLUTION: 7
    POOLER_SAMPLING_RATIO: 2
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    PREDICTOR: FPNPredictor
    USE_GN: False
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)
    BG_IOU_THRESHOLD: 0.3
    DETECTIONS_PER_IMG: 80
    FG_IOU_THRESHOLD: 0.5
    NMS: 0.3
    NMS_FILTER_DUPLICATES: True
    POSITIVE_FRACTION: 0.5
    POST_NMS_PER_CLS_TOPN: 300
    SCORE_THRESH: 0.01
    USE_FPN: True
  ROI_KEYPOINT_HEAD:
    CONV_LAYERS: (512, 512, 512, 512, 512, 512, 512, 512)
    FEATURE_EXTRACTOR: KeypointRCNNFeatureExtractor
    MLP_HEAD_DIM: 1024
    NUM_CLASSES: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    PREDICTOR: KeypointRCNNPredictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
  ROI_MASK_HEAD:
    CONV_LAYERS: (256, 256, 256, 256)
    DILATION: 1
    FEATURE_EXTRACTOR: ResNet50Conv5ROIFeatureExtractor
    MLP_HEAD_DIM: 1024
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_SCALES: (0.0625,)
    POSTPROCESS_MASKS: False
    POSTPROCESS_MASKS_THRESHOLD: 0.5
    PREDICTOR: MaskRCNNC4Predictor
    RESOLUTION: 14
    SHARE_BOX_FEATURE_EXTRACTOR: True
    USE_GN: False
  ROI_RELATION_HEAD:
    ADD_GTBOX_TO_PROPOSAL_IN_TRAIN: True
    BATCH_SIZE_PER_IMAGE: 1024
    CAUSAL:
      CONTEXT_LAYER: motifs
      EFFECT_ANALYSIS: True
      EFFECT_TYPE: none
      FUSION_TYPE: sum
      SEPARATE_SPATIAL: False
      SPATIAL_FOR_VISION: True
    CONTEXT_DROPOUT_RATE: 0.2
    CONTEXT_HIDDEN_DIM: 512
    CONTEXT_OBJ_LAYER: 1
    CONTEXT_POOLING_DIM: 4096
    CONTEXT_REL_LAYER: 1
    EBM:
      OBJ_EMBED_DIM: 512
      OBJ_LABEL_EMBED_DIM: 512
      POOLING_DIM: 512
      REL_EMBED_DIM: 512
      REL_LABEL_EMBED_DIM: 512
    EMBED_DIM: 200
    FEATURE_EXTRACTOR: RelationFeatureExtractor
    LABEL_SMOOTHING_LOSS: False
    NUM_CLASSES: 51
    NUM_SAMPLE_PER_GT_REL: 4
    POOLING_ALL_LEVELS: True
    POSITIVE_FRACTION: 0.25
    PREDICTOR: TransformerPredictor
    PREDICT_USE_BIAS: True
    PREDICT_USE_VISION: True
    REL_PROP: [0.01858, 0.00057, 0.00051, 0.00109, 0.0015, 0.00489, 0.00432, 0.02913, 0.00245, 0.00121, 0.00404, 0.0011, 0.00132, 0.00172, 5e-05, 0.00242, 0.0005, 0.00048, 0.00208, 0.15608, 0.0265, 0.06091, 0.009, 0.00183, 0.00225, 0.0009, 0.00028, 0.00077, 0.04844, 0.08645, 0.31621, 0.00088, 0.00301, 0.00042, 0.00186, 0.001, 0.00027, 0.01012, 0.0001, 0.01286, 0.00647, 0.00084, 0.01077, 0.00132, 0.00069, 0.00376, 0.00214, 0.11424, 0.01205, 0.02958]
    REQUIRE_BOX_OVERLAP: False
    TRANSFORMER:
      DROPOUT_RATE: 0.1
      INNER_DIM: 2048
      KEY_DIM: 64
      NUM_HEAD: 8
      OBJ_LAYER: 4
      REL_LAYER: 2
      VAL_DIM: 64
    USE_GT_BOX: True
    USE_GT_OBJECT_LABEL: True
  RPN:
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ASPECT_RATIOS: (0.23232838, 0.63365731, 1.28478321, 3.15089189)
    BATCH_SIZE_PER_IMAGE: 256
    BG_IOU_THRESHOLD: 0.3
    FG_IOU_THRESHOLD: 0.7
    FPN_POST_NMS_PER_BATCH: False
    FPN_POST_NMS_TOP_N_TEST: 1000
    FPN_POST_NMS_TOP_N_TRAIN: 1000
    MIN_SIZE: 0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOP_N_TEST: 1000
    POST_NMS_TOP_N_TRAIN: 1000
    PRE_NMS_TOP_N_TEST: 6000
    PRE_NMS_TOP_N_TRAIN: 6000
    RPN_HEAD: SingleConvRPNHead
    RPN_MID_CHANNEL: 256
    STRADDLE_THRESH: 0
    USE_FPN: True
  RPN_ONLY: False
  VGG:
    VGG16_OUT_CHANNELS: 512
  WEIGHT: catalog://ImageNetPretrained/FAIR/20171220/X-101-32x8d
OUTPUT_DIR: /users/students/r0879687/amager/vg/output/relation_baseline
PATHS_CATALOG: /users/students/r0879687/Documents/Master_thesis/ebm/energy-based-scene-graph/maskrcnn_benchmark/config/paths_catalog.py
PATHS_DATA: /users/students/r0879687/amager/vg/VG_100K
SAMPLER:
  GRAD_CLIP: 0.01
  ITERS: 10
  LR: 0.1
  NAME: SGLD
  VAR: 0.001
SOLVER:
  BASE_LR: 0.0025
  BIAS_LR_FACTOR: 1
  CHECKPOINT_PERIOD: 2000
  CLIP_NORM: 5.0
  GAMMA: 0.1
  GRAD_NORM_CLIP: 5.0
  IMS_PER_BATCH: 1
  MAX_ITER: 60000
  MOMENTUM: 0.9
  PRE_VAL: False
  PRINT_GRAD_FREQ: 4000
  SCHEDULE:
    COOLDOWN: 0
    FACTOR: 0.05
    MAX_DECAY_STEP: 3
    PATIENCE: 2
    THRESHOLD: 0.001
    TYPE: WarmupMultiStepLR
  STEPS: (0, 30000, 40000)
  TO_VAL: True
  UPDATE_SCHEDULE_DURING_LOAD: False
  VAL_PERIOD: 2000
  WARMUP_FACTOR: 0.1
  WARMUP_ITERS: 500
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: 0.0
TEST:
  ALLOW_LOAD_FROM_CACHE: False
  BBOX_AUG:
    ENABLED: False
    H_FLIP: False
    MAX_SIZE: 4000
    SCALES: ()
    SCALE_H_FLIP: False
  DETECTIONS_PER_IMG: 100
  EXPECTED_RESULTS: []
  EXPECTED_RESULTS_SIGMA_TOL: 4
  IMS_PER_BATCH: 1
  RELATION:
    IOU_THRESHOLD: 0.5
    LATER_NMS_PREDICTION_THRES: 0.5
    MULTIPLE_PREDS: False
    REQUIRE_OVERLAP: False
    SYNC_GATHER: True
  SAVE_PROPOSALS: False
WANDB:
  MUTE: False
2023-02-27 10:01:11,600 maskrcnn_benchmark INFO: Saving config into: /users/students/r0879687/amager/vg/output/relation_baseline/config.yml
2023-02-27 10:01:11,679 maskrcnn_benchmark INFO: #################### prepare training ####################
2023-02-27 10:01:11,680 maskrcnn_benchmark.modeling.detector.generalized_rcnn INFO: #################### Building Backbone ####################
2023-02-27 10:01:12,942 maskrcnn_benchmark.modeling.detector.generalized_rcnn INFO: #################### Building RPN ####################
2023-02-27 10:01:12,952 maskrcnn_benchmark.modeling.detector.generalized_rcnn INFO: #################### Building ROI Heads ####################
2023-02-27 10:01:15,747 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
2023-02-27 10:01:15,747 maskrcnn_benchmark.data.build INFO: get dataset statistics...
2023-02-27 10:01:15,749 maskrcnn_benchmark.data.build INFO: Loading data statistics from: /users/students/r0879687/amager/vg/output/relation_baseline/VG_stanford_filtered_with_attribute_train_statistics.cache
2023-02-27 10:01:15,750 maskrcnn_benchmark.data.build INFO: ----------------------------------------------------------------------------------------------------
loading word vectors from /users/students/r0879687/amager/vg/glove/glove.6B.200d.pt
__background__ -> __background__ 
fail on __background__
2023-02-27 10:01:34,298 maskrcnn_benchmark INFO: #################### end model construction ####################
2023-02-27 10:01:34,562 maskrcnn_benchmark INFO: #################### end optimizer and shcedule ####################
2023-02-27 10:01:34,562 maskrcnn_benchmark INFO: #################### end distributed ####################
2023-02-27 10:01:34,564 maskrcnn_benchmark.utils.checkpoint INFO: Loading checkpoint from /users/students/r0879687/amager/vg/pretrained-detector/pretrained_faster_rcnn/model_final.pth
2023-02-27 10:01:52,155 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
2023-02-27 10:01:52,155 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
2023-02-27 10:01:52,155 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
2023-02-27 10:01:52,155 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.box_feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
2023-02-27 10:01:52,156 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias in current model to roi_heads.box.feature_extractor.fc6.bias in loaded model.
2023-02-27 10:01:52,156 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight in current model to roi_heads.box.feature_extractor.fc6.weight in loaded model.
2023-02-27 10:01:52,156 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias in current model to roi_heads.box.feature_extractor.fc7.bias in loaded model.
2023-02-27 10:01:52,156 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight in current model to roi_heads.box.feature_extractor.fc7.weight in loaded model.
2023-02-27 10:01:52,156 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.bias in loaded model.
2023-02-27 10:01:52,156 maskrcnn_benchmark.utils.model_serialization INFO: MAPPING roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight in current model to roi_heads.box.feature_extractor.pooler.reduce_channel.0.weight in loaded model.
2023-02-27 10:01:52,218 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc6.bias                                                loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
2023-02-27 10:01:52,218 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc6.weight                                              loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
2023-02-27 10:01:52,218 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc7.bias                                                loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
2023-02-27 10:01:52,219 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.box_feature_extractor.fc7.weight                                              loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
2023-02-27 10:01:52,219 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.0.bias of shape (32,)
2023-02-27 10:01:52,219 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.0.weight of shape (32, 9)
2023-02-27 10:01:52,219 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.3.bias of shape (128,)
2023-02-27 10:01:52,219 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.bbox_embed.3.weight of shape (128, 32)
2023-02-27 10:01:52,219 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.layer_norm.bias of shape (512,)
2023-02-27 10:01:52,219 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.layer_norm.weight of shape (512,)
2023-02-27 10:01:52,219 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_1.bias of shape (2048,)
2023-02-27 10:01:52,220 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_1.weight of shape (2048, 512, 1)
2023-02-27 10:01:52,220 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_2.bias of shape (512,)
2023-02-27 10:01:52,220 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_2.weight of shape (512, 2048, 1)
2023-02-27 10:01:52,220 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.fc.bias of shape (512,)
2023-02-27 10:01:52,220 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.fc.weight of shape (512, 512)
2023-02-27 10:01:52,220 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.layer_norm.bias of shape (512,)
2023-02-27 10:01:52,220 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.layer_norm.weight of shape (512,)
2023-02-27 10:01:52,220 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_ks.bias of shape (512,)
2023-02-27 10:01:52,220 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_ks.weight of shape (512, 512)
2023-02-27 10:01:52,220 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_qs.bias of shape (512,)
2023-02-27 10:01:52,220 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_qs.weight of shape (512, 512)
2023-02-27 10:01:52,220 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_vs.bias of shape (512,)
2023-02-27 10:01:52,220 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_vs.weight of shape (512, 512)
2023-02-27 10:01:52,220 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.layer_norm.bias of shape (512,)
2023-02-27 10:01:52,221 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.layer_norm.weight of shape (512,)
2023-02-27 10:01:52,221 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_1.bias of shape (2048,)
2023-02-27 10:01:52,221 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_1.weight of shape (2048, 512, 1)
2023-02-27 10:01:52,221 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_2.bias of shape (512,)
2023-02-27 10:01:52,221 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_2.weight of shape (512, 2048, 1)
2023-02-27 10:01:52,221 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.fc.bias of shape (512,)
2023-02-27 10:01:52,221 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.fc.weight of shape (512, 512)
2023-02-27 10:01:52,221 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.layer_norm.bias of shape (512,)
2023-02-27 10:01:52,221 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.layer_norm.weight of shape (512,)
2023-02-27 10:01:52,221 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_ks.bias of shape (512,)
2023-02-27 10:01:52,221 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_ks.weight of shape (512, 512)
2023-02-27 10:01:52,221 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_qs.bias of shape (512,)
2023-02-27 10:01:52,221 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_qs.weight of shape (512, 512)
2023-02-27 10:01:52,221 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_vs.bias of shape (512,)
2023-02-27 10:01:52,222 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_vs.weight of shape (512, 512)
2023-02-27 10:01:52,222 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.bias of shape (512,)
2023-02-27 10:01:52,222 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.weight of shape (512,)
2023-02-27 10:01:52,222 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.bias of shape (2048,)
2023-02-27 10:01:52,222 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.weight of shape (2048, 512, 1)
2023-02-27 10:01:52,222 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.bias of shape (512,)
2023-02-27 10:01:52,222 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.weight of shape (512, 2048, 1)
2023-02-27 10:01:52,222 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.bias of shape (512,)
2023-02-27 10:01:52,222 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.weight of shape (512, 512)
2023-02-27 10:01:52,222 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.bias of shape (512,)
2023-02-27 10:01:52,222 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.weight of shape (512,)
2023-02-27 10:01:52,222 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.bias of shape (512,)
2023-02-27 10:01:52,222 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.weight of shape (512, 512)
2023-02-27 10:01:52,222 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.bias of shape (512,)
2023-02-27 10:01:52,223 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.weight of shape (512, 512)
2023-02-27 10:01:52,223 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.bias of shape (512,)
2023-02-27 10:01:52,223 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.weight of shape (512, 512)
2023-02-27 10:01:52,223 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.bias of shape (512,)
2023-02-27 10:01:52,223 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.weight of shape (512,)
2023-02-27 10:01:52,223 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.bias of shape (2048,)
2023-02-27 10:01:52,223 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.weight of shape (2048, 512, 1)
2023-02-27 10:01:52,223 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.bias of shape (512,)
2023-02-27 10:01:52,223 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.weight of shape (512, 2048, 1)
2023-02-27 10:01:52,223 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.bias of shape (512,)
2023-02-27 10:01:52,223 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.weight of shape (512, 512)
2023-02-27 10:01:52,223 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.bias of shape (512,)
2023-02-27 10:01:52,223 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.weight of shape (512,)
2023-02-27 10:01:52,223 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.bias of shape (512,)
2023-02-27 10:01:52,224 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.weight of shape (512, 512)
2023-02-27 10:01:52,224 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.bias of shape (512,)
2023-02-27 10:01:52,224 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.weight of shape (512, 512)
2023-02-27 10:01:52,224 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.bias of shape (512,)
2023-02-27 10:01:52,224 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.weight of shape (512, 512)
2023-02-27 10:01:52,224 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.bias of shape (512,)
2023-02-27 10:01:52,224 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.weight of shape (512,)
2023-02-27 10:01:52,224 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.bias of shape (2048,)
2023-02-27 10:01:52,224 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.weight of shape (2048, 512, 1)
2023-02-27 10:01:52,224 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.bias of shape (512,)
2023-02-27 10:01:52,224 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.weight of shape (512, 2048, 1)
2023-02-27 10:01:52,224 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.bias of shape (512,)
2023-02-27 10:01:52,224 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.weight of shape (512, 512)
2023-02-27 10:01:52,225 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.bias of shape (512,)
2023-02-27 10:01:52,225 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.weight of shape (512,)
2023-02-27 10:01:52,225 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.bias of shape (512,)
2023-02-27 10:01:52,225 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.weight of shape (512, 512)
2023-02-27 10:01:52,225 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.bias of shape (512,)
2023-02-27 10:01:52,225 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.weight of shape (512, 512)
2023-02-27 10:01:52,225 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.bias of shape (512,)
2023-02-27 10:01:52,225 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.weight of shape (512, 512)
2023-02-27 10:01:52,225 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.bias of shape (512,)
2023-02-27 10:01:52,225 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.weight of shape (512,)
2023-02-27 10:01:52,225 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.bias of shape (2048,)
2023-02-27 10:01:52,225 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.weight of shape (2048, 512, 1)
2023-02-27 10:01:52,225 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.bias of shape (512,)
2023-02-27 10:01:52,226 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.weight of shape (512, 2048, 1)
2023-02-27 10:01:52,226 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.bias of shape (512,)
2023-02-27 10:01:52,226 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.weight of shape (512, 512)
2023-02-27 10:01:52,226 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.bias of shape (512,)
2023-02-27 10:01:52,226 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.weight of shape (512,)
2023-02-27 10:01:52,226 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.bias of shape (512,)
2023-02-27 10:01:52,226 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.weight of shape (512, 512)
2023-02-27 10:01:52,226 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.bias of shape (512,)
2023-02-27 10:01:52,226 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.weight of shape (512, 512)
2023-02-27 10:01:52,226 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.bias of shape (512,)
2023-02-27 10:01:52,226 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.weight of shape (512, 512)
2023-02-27 10:01:52,226 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge.bias of shape (512,)
2023-02-27 10:01:52,226 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_edge.weight of shape (512, 4808)
2023-02-27 10:01:52,226 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj.bias of shape (512,)
2023-02-27 10:01:52,227 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.lin_obj.weight of shape (512, 4424)
2023-02-27 10:01:52,227 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed1.weight of shape (151, 200)
2023-02-27 10:01:52,227 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.obj_embed2.weight of shape (151, 200)
2023-02-27 10:01:52,227 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.out_obj.bias of shape (151,)
2023-02-27 10:01:52,227 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.context_layer.out_obj.weight of shape (151, 512)
2023-02-27 10:01:52,227 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.bias of shape (51,)
2023-02-27 10:01:52,227 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.ctx_compress.weight of shape (51, 1024)
2023-02-27 10:01:52,227 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.freq_bias.obj_baseline.weight of shape (22801, 51)
2023-02-27 10:01:52,227 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.bias of shape (4096,)
2023-02-27 10:01:52,227 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.post_cat.weight of shape (4096, 1024)
2023-02-27 10:01:52,227 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.post_emb.bias of shape (1024,)
2023-02-27 10:01:52,227 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.post_emb.weight of shape (1024, 512)
2023-02-27 10:01:52,227 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress.bias of shape (51,)
2023-02-27 10:01:52,227 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.predictor.rel_compress.weight of shape (51, 4096)
2023-02-27 10:01:52,228 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias                            loaded from roi_heads.box.feature_extractor.fc6.bias         of shape (4096,)
2023-02-27 10:01:52,228 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight                          loaded from roi_heads.box.feature_extractor.fc6.weight       of shape (4096, 12544)
2023-02-27 10:01:52,228 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias                            loaded from roi_heads.box.feature_extractor.fc7.bias         of shape (4096,)
2023-02-27 10:01:52,228 maskrcnn_benchmark.utils.model_serialization INFO: REMATCHING! roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight                          loaded from roi_heads.box.feature_extractor.fc7.weight       of shape (4096, 4096)
2023-02-27 10:01:52,228 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias of shape (256,)
2023-02-27 10:01:52,228 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight of shape (256, 1024, 3, 3)
2023-02-27 10:01:52,228 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.bias of shape (128,)
2023-02-27 10:01:52,228 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.0.weight of shape (128, 2, 7, 7)
2023-02-27 10:01:52,228 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.bias of shape (128,)
2023-02-27 10:01:52,228 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.num_batches_tracked of shape ()
2023-02-27 10:01:52,228 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_mean of shape (128,)
2023-02-27 10:01:52,228 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.running_var of shape (128,)
2023-02-27 10:01:52,228 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.2.weight of shape (128,)
2023-02-27 10:01:52,229 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.bias of shape (256,)
2023-02-27 10:01:52,229 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.4.weight of shape (256, 128, 3, 3)
2023-02-27 10:01:52,229 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.bias of shape (256,)
2023-02-27 10:01:52,229 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.num_batches_tracked of shape ()
2023-02-27 10:01:52,229 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_mean of shape (256,)
2023-02-27 10:01:52,229 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.running_var of shape (256,)
2023-02-27 10:01:52,229 maskrcnn_benchmark.utils.model_serialization INFO: NO-MATCHING of current module: roi_heads.relation.union_feature_extractor.rect_conv.6.weight of shape (256,)
2023-02-27 10:01:52,555 maskrcnn_benchmark INFO: #################### end load checkpointer ####################
2023-02-27 10:03:27,105 maskrcnn_benchmark.utils.miscellaneous INFO: Saving labels mapping into /users/students/r0879687/amager/vg/output/relation_baseline/labels.json
2023-02-27 10:04:02,485 maskrcnn_benchmark INFO: #################### end dataloader ####################
2023-02-27 10:04:02,485 maskrcnn_benchmark INFO: Start training
2023-02-27 10:04:21,955 maskrcnn_benchmark INFO: ---Total norm 2694.01733 clip coef 0.00186-----------------
2023-02-27 10:04:21,969 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.lin_edge.weight: 1689.01038, (torch.Size([512, 4808]))
2023-02-27 10:04:21,969 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.lin_obj.weight: 1255.63684, (torch.Size([512, 4424]))
2023-02-27 10:04:21,969 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc7.weight: 946.61371, (torch.Size([4096, 4096]))
2023-02-27 10:04:21,969 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc6.weight: 878.27454, (torch.Size([4096, 12544]))
2023-02-27 10:04:21,969 maskrcnn_benchmark INFO: roi_heads.relation.predictor.ctx_compress.weight  : 322.38229, (torch.Size([51, 1024]))
2023-02-27 10:04:21,969 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_2.weight: 279.91467, (torch.Size([512, 2048, 1]))
2023-02-27 10:04:21,969 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_2.weight: 279.02026, (torch.Size([512, 2048, 1]))
2023-02-27 10:04:21,970 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.fc.weight: 268.91061, (torch.Size([512, 512]))
2023-02-27 10:04:21,970 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.fc.weight: 261.07077, (torch.Size([512, 512]))
2023-02-27 10:04:21,970 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 235.05629, (torch.Size([4096, 12544]))
2023-02-27 10:04:21,970 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.weight: 232.70903, (torch.Size([512, 2048, 1]))
2023-02-27 10:04:21,970 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.weight: 229.15025, (torch.Size([512, 2048, 1]))
2023-02-27 10:04:21,970 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.weight: 222.68742, (torch.Size([512, 512]))
2023-02-27 10:04:21,970 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.weight: 219.82419, (torch.Size([512, 512]))
2023-02-27 10:04:21,970 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.weight: 216.57263, (torch.Size([512, 2048, 1]))
2023-02-27 10:04:21,970 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.weight: 213.55989, (torch.Size([512, 2048, 1]))
2023-02-27 10:04:21,970 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.weight: 208.91508, (torch.Size([512, 512]))
2023-02-27 10:04:21,971 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_vs.weight: 201.61742, (torch.Size([512, 512]))
2023-02-27 10:04:21,971 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_vs.weight: 193.81990, (torch.Size([512, 512]))
2023-02-27 10:04:21,971 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 182.68735, (torch.Size([4096, 4096]))
2023-02-27 10:04:21,971 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.weight: 175.39687, (torch.Size([512, 512]))
2023-02-27 10:04:21,971 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.weight: 160.58749, (torch.Size([512, 512]))
2023-02-27 10:04:21,971 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.weight: 155.43132, (torch.Size([512, 512]))
2023-02-27 10:04:21,971 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.weight: 148.19653, (torch.Size([512, 512]))
2023-02-27 10:04:21,971 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.weight: 131.50136, (torch.Size([512, 512]))
2023-02-27 10:04:21,971 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_1.weight: 121.88994, (torch.Size([2048, 512, 1]))
2023-02-27 10:04:21,971 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_1.weight: 118.46215, (torch.Size([2048, 512, 1]))
2023-02-27 10:04:21,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_cat.weight      : 104.87276, (torch.Size([4096, 1024]))
2023-02-27 10:04:21,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.weight: 100.58656, (torch.Size([2048, 512, 1]))
2023-02-27 10:04:21,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.rel_compress.weight  : 98.23090, (torch.Size([51, 4096]))
2023-02-27 10:04:21,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.weight: 97.99936, (torch.Size([2048, 512, 1]))
2023-02-27 10:04:21,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.weight: 97.63675, (torch.Size([2048, 512, 1]))
2023-02-27 10:04:21,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.weight: 93.51900, (torch.Size([2048, 512, 1]))
2023-02-27 10:04:21,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.lin_obj.bias: 90.11815, (torch.Size([512]))
2023-02-27 10:04:21,972 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 70.75938, (torch.Size([256, 1024, 3, 3]))
2023-02-27 10:04:21,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.lin_edge.bias: 68.38799, (torch.Size([512]))
2023-02-27 10:04:21,972 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.bias: 54.04202, (torch.Size([512]))
2023-02-27 10:04:21,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.bias: 51.85494, (torch.Size([512]))
2023-02-27 10:04:21,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.weight: 46.26301, (torch.Size([512, 512]))
2023-02-27 10:04:21,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.weight: 45.56255, (torch.Size([512, 512]))
2023-02-27 10:04:21,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_vs.bias: 40.60476, (torch.Size([512]))
2023-02-27 10:04:21,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.fc.bias: 39.19009, (torch.Size([512]))
2023-02-27 10:04:21,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.weight: 33.70642, (torch.Size([512, 512]))
2023-02-27 10:04:21,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.weight: 33.35017, (torch.Size([512, 512]))
2023-02-27 10:04:21,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_emb.weight      : 32.52232, (torch.Size([1024, 512]))
2023-02-27 10:04:21,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_qs.weight: 32.23300, (torch.Size([512, 512]))
2023-02-27 10:04:21,973 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_ks.weight: 32.07987, (torch.Size([512, 512]))
2023-02-27 10:04:21,974 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 30.44895, (torch.Size([256, 128, 3, 3]))
2023-02-27 10:04:21,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.weight: 19.54343, (torch.Size([512, 512]))
2023-02-27 10:04:21,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.weight: 18.03717, (torch.Size([512, 512]))
2023-02-27 10:04:21,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.layer_norm.weight: 17.63209, (torch.Size([512]))
2023-02-27 10:04:21,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.layer_norm.weight: 17.58638, (torch.Size([512]))
2023-02-27 10:04:21,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.layer_norm.bias: 16.98697, (torch.Size([512]))
2023-02-27 10:04:21,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.layer_norm.bias: 16.63973, (torch.Size([512]))
2023-02-27 10:04:21,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_2.bias: 16.38305, (torch.Size([512]))
2023-02-27 10:04:21,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.layer_norm.bias: 16.10711, (torch.Size([512]))
2023-02-27 10:04:21,974 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.layer_norm.bias: 15.99026, (torch.Size([512]))
2023-02-27 10:04:21,975 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_2.bias: 15.80623, (torch.Size([512]))
2023-02-27 10:04:21,975 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.layer_norm.weight: 15.56581, (torch.Size([512]))
2023-02-27 10:04:21,975 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.layer_norm.weight: 14.87492, (torch.Size([512]))
2023-02-27 10:04:21,975 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.bias: 14.85979, (torch.Size([512]))
2023-02-27 10:04:21,975 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc7.bias : 14.78026, (torch.Size([4096]))
2023-02-27 10:04:21,975 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.bias: 14.75223, (torch.Size([512]))
2023-02-27 10:04:21,975 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.bias: 14.40527, (torch.Size([512]))
2023-02-27 10:04:21,975 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.bias: 14.13812, (torch.Size([512]))
2023-02-27 10:04:21,975 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.bias: 14.09988, (torch.Size([512]))
2023-02-27 10:04:21,975 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.bias: 13.85074, (torch.Size([512]))
2023-02-27 10:04:21,976 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.bias: 13.28070, (torch.Size([512]))
2023-02-27 10:04:21,976 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.bias: 13.04169, (torch.Size([512]))
2023-02-27 10:04:21,976 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.bias: 13.02645, (torch.Size([512]))
2023-02-27 10:04:21,976 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.bias: 12.82266, (torch.Size([512]))
2023-02-27 10:04:21,976 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.bias: 12.76477, (torch.Size([512]))
2023-02-27 10:04:21,976 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.bias: 12.66843, (torch.Size([512]))
2023-02-27 10:04:21,976 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.weight: 12.51230, (torch.Size([512]))
2023-02-27 10:04:21,976 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.weight: 12.22106, (torch.Size([512]))
2023-02-27 10:04:21,976 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.weight: 12.08476, (torch.Size([512]))
2023-02-27 10:04:21,976 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.weight: 11.48245, (torch.Size([512]))
2023-02-27 10:04:21,976 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 11.31328, (torch.Size([128, 2, 7, 7]))
2023-02-27 10:04:21,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.weight: 10.99885, (torch.Size([512]))
2023-02-27 10:04:21,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.weight: 10.98914, (torch.Size([512]))
2023-02-27 10:04:21,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.weight: 10.98157, (torch.Size([512]))
2023-02-27 10:04:21,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.weight: 10.94407, (torch.Size([512]))
2023-02-27 10:04:21,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.fc.bias: 10.88154, (torch.Size([512]))
2023-02-27 10:04:21,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_vs.bias: 10.28309, (torch.Size([512]))
2023-02-27 10:04:21,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.bias: 9.03000, (torch.Size([512]))
2023-02-27 10:04:21,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.bias: 8.85458, (torch.Size([512]))
2023-02-27 10:04:21,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.bias: 8.55128, (torch.Size([512]))
2023-02-27 10:04:21,977 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.bias: 8.41867, (torch.Size([512]))
2023-02-27 10:04:21,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.bias: 7.82594, (torch.Size([512]))
2023-02-27 10:04:21,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.bias: 7.68306, (torch.Size([512]))
2023-02-27 10:04:21,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_ks.weight: 6.42648, (torch.Size([512, 512]))
2023-02-27 10:04:21,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_qs.weight: 6.22200, (torch.Size([512, 512]))
2023-02-27 10:04:21,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_1.bias: 6.10485, (torch.Size([2048]))
2023-02-27 10:04:21,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_1.bias: 5.81245, (torch.Size([2048]))
2023-02-27 10:04:21,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.bias: 5.27584, (torch.Size([2048]))
2023-02-27 10:04:21,978 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc6.bias : 5.13186, (torch.Size([4096]))
2023-02-27 10:04:21,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.bias: 4.96580, (torch.Size([2048]))
2023-02-27 10:04:21,978 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_embed1.weight: 4.80298, (torch.Size([151, 200]))
2023-02-27 10:04:21,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.bias: 4.78377, (torch.Size([2048]))
2023-02-27 10:04:21,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.bias: 4.56207, (torch.Size([2048]))
2023-02-27 10:04:21,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 4.56097, (torch.Size([128]))
2023-02-27 10:04:21,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.weight: 4.35400, (torch.Size([512, 512]))
2023-02-27 10:04:21,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.weight: 4.19240, (torch.Size([512, 512]))
2023-02-27 10:04:21,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 3.08047, (torch.Size([128, 32]))
2023-02-27 10:04:21,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_embed2.weight: 3.06553, (torch.Size([151, 200]))
2023-02-27 10:04:21,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.bias: 2.33730, (torch.Size([512]))
2023-02-27 10:04:21,979 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 2.16388, (torch.Size([32, 9]))
2023-02-27 10:04:21,979 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 2.03385, (torch.Size([4096]))
2023-02-27 10:04:21,980 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.bias: 1.67493, (torch.Size([512]))
2023-02-27 10:04:21,980 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 1.63970, (torch.Size([128]))
2023-02-27 10:04:21,980 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_emb.bias        : 1.61291, (torch.Size([1024]))
2023-02-27 10:04:21,980 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_qs.bias: 1.61118, (torch.Size([512]))
2023-02-27 10:04:21,980 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 1.51631, (torch.Size([32]))
2023-02-27 10:04:21,980 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 1.38781, (torch.Size([256]))
2023-02-27 10:04:21,980 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_qs.bias: 1.20088, (torch.Size([512]))
2023-02-27 10:04:21,980 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.bias: 1.18526, (torch.Size([512]))
2023-02-27 10:04:21,980 maskrcnn_benchmark INFO: roi_heads.relation.predictor.rel_compress.bias    : 1.13308, (torch.Size([51]))
2023-02-27 10:04:21,980 maskrcnn_benchmark INFO: roi_heads.relation.predictor.ctx_compress.bias    : 1.13308, (torch.Size([51]))
2023-02-27 10:04:21,981 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.bias: 0.94973, (torch.Size([512]))
2023-02-27 10:04:21,981 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.89587, (torch.Size([4096]))
2023-02-27 10:04:21,981 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.82176, (torch.Size([256]))
2023-02-27 10:04:21,981 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.53532, (torch.Size([128]))
2023-02-27 10:04:21,981 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.49515, (torch.Size([256]))
2023-02-27 10:04:21,981 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.42662, (torch.Size([256]))
2023-02-27 10:04:21,981 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_cat.bias        : 0.36619, (torch.Size([4096]))
2023-02-27 10:04:21,981 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.29783, (torch.Size([128]))
2023-02-27 10:04:21,981 maskrcnn_benchmark INFO: roi_heads.relation.predictor.freq_bias.obj_baseline.weight: 0.18492, (torch.Size([22801, 51]))
2023-02-27 10:04:21,981 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2023-02-27 10:04:21,982 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2023-02-27 10:04:21,982 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2023-02-27 10:04:21,982 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2023-02-27 10:04:21,982 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2023-02-27 10:04:21,982 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2023-02-27 10:04:21,982 maskrcnn_benchmark INFO: -------------------------------
2023-02-27 10:05:13,635 maskrcnn_benchmark INFO: eta: 5:54:33  iter: 200  loss: 5.2320 (9.7212)  loss_rel: 1.2034 (5.6926)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2577 (0.3557)  data: 0.0027 (0.0090)  lr: 0.000115  max mem: 3718
2023-02-27 10:06:06,189 maskrcnn_benchmark INFO: eta: 5:07:11  iter: 400  loss: 4.8662 (7.7249)  loss_rel: 0.8376 (3.6963)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2491 (0.3093)  data: 0.0027 (0.0059)  lr: 0.000205  max mem: 4002
2023-02-27 10:06:58,789 maskrcnn_benchmark INFO: eta: 4:50:53  iter: 600  loss: 4.7387 (6.9832)  loss_rel: 0.7101 (2.9546)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2530 (0.2938)  data: 0.0027 (0.0048)  lr: 0.000250  max mem: 4002
2023-02-27 10:07:51,072 maskrcnn_benchmark INFO: eta: 4:41:55  iter: 800  loss: 4.7590 (6.5352)  loss_rel: 0.7304 (2.5067)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2463 (0.2857)  data: 0.0028 (0.0043)  lr: 0.000250  max mem: 4002
2023-02-27 10:08:43,110 maskrcnn_benchmark INFO: eta: 4:35:56  iter: 1000  loss: 4.5872 (6.2350)  loss_rel: 0.5586 (2.2064)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2564 (0.2806)  data: 0.0028 (0.0040)  lr: 0.000250  max mem: 4002
2023-02-27 10:09:36,011 maskrcnn_benchmark INFO: eta: 4:32:22  iter: 1200  loss: 4.4599 (5.9997)  loss_rel: 0.4313 (1.9711)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2574 (0.2779)  data: 0.0027 (0.0038)  lr: 0.000250  max mem: 4002
2023-02-27 10:10:28,371 maskrcnn_benchmark INFO: eta: 4:29:12  iter: 1400  loss: 4.4401 (5.8377)  loss_rel: 0.4115 (1.8091)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2596 (0.2756)  data: 0.0029 (0.0037)  lr: 0.000250  max mem: 4002
2023-02-27 10:11:20,590 maskrcnn_benchmark INFO: eta: 4:26:30  iter: 1600  loss: 4.5922 (5.7253)  loss_rel: 0.5636 (1.6968)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2427 (0.2738)  data: 0.0027 (0.0035)  lr: 0.000250  max mem: 4002
2023-02-27 10:12:13,611 maskrcnn_benchmark INFO: eta: 4:24:39  iter: 1800  loss: 4.6306 (5.6171)  loss_rel: 0.6020 (1.5885)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2487 (0.2728)  data: 0.0027 (0.0035)  lr: 0.000250  max mem: 4003
2023-02-27 10:13:05,425 maskrcnn_benchmark INFO: eta: 4:22:25  iter: 2000  loss: 4.3730 (5.5463)  loss_rel: 0.3444 (1.5177)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2538 (0.2715)  data: 0.0028 (0.0034)  lr: 0.000250  max mem: 4006
2023-02-27 10:13:05,428 maskrcnn_benchmark.utils.checkpoint INFO: Saving checkpoint to /users/students/r0879687/amager/vg/output/relation_baseline/model_0002000.pth
2023-02-27 10:13:49,376 maskrcnn_benchmark INFO: Start validating
2023-02-27 10:13:49,379 maskrcnn_benchmark INFO: Start evaluation on VG_stanford_filtered_with_attribute_val dataset(5000 images).
2023-02-27 10:30:12,132 maskrcnn_benchmark INFO: Total run time: 0:16:22.753408 (0.19655068159103395 s / img per device, on 1 devices)
2023-02-27 10:30:12,133 maskrcnn_benchmark INFO: Model inference time: 0:15:59.036205 (0.19180724105834962 s / img per device, on 1 devices)
creating index...
index created!
Loading and preparing results...
Converting ndarray to lists...
(62754, 7)
0/62754
DONE (t=0.32s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=24.17s).
Accumulating evaluation results...
DONE (t=5.17s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.675
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.994
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000
2023-02-27 10:33:53,269 maskrcnn_benchmark INFO: 
====================================================================================================
Detection evaluation mAp=0.9999
====================================================================================================
SGG eval:   R @ 20: 0.4817;   R @ 50: 0.5434;   R @ 100: 0.5679;  for mode=predcls, type=Recall(Main).
SGG eval: ngR @ 20: 0.5297; ngR @ 50: 0.6543; ngR @ 100: 0.7329;  for mode=predcls, type=No Graph Constraint Recall(Main).
SGG eval:  zR @ 20: 0.0156;  zR @ 50: 0.0511;  zR @ 100: 0.0815;  for mode=predcls, type=Zero Shot Recall.
SGG eval:  zR @ 20: 0.0634;  zR @ 50: 0.0936;  zR @ 100: 0.1126;  for mode=predcls, type=1 Shot Recall.
 zR @ 20: 0.0601;  zR @ 50: 0.0856;  zR @ 100: 0.0991;  for mode=predcls, type=2 Shot Recall.
 zR @ 20: 0.0824;  zR @ 50: 0.1227;  zR @ 100: 0.1410;  for mode=predcls, type=3 Shot Recall.
 zR @ 20: 0.1122;  zR @ 50: 0.1727;  zR @ 100: 0.2006;  for mode=predcls, type=4 Shot Recall.
 zR @ 20: 0.1279;  zR @ 50: 0.1535;  zR @ 100: 0.1689;  for mode=predcls, type=5 Shot Recall.
 zR @ 20: 0.1513;  zR @ 50: 0.2120;  zR @ 100: 0.2564;  for mode=predcls, type=6 Shot Recall.
 zR @ 20: 0.1028;  zR @ 50: 0.1389;  zR @ 100: 0.1639;  for mode=predcls, type=7 Shot Recall.
 zR @ 20: 0.1394;  zR @ 50: 0.1758;  zR @ 100: 0.1924;  for mode=predcls, type=8 Shot Recall.
 zR @ 20: 0.1734;  zR @ 50: 0.2806;  zR @ 100: 0.3129;  for mode=predcls, type=9 Shot Recall.
 zR @ 20: 0.1498;  zR @ 50: 0.1849;  zR @ 100: 0.2277;  for mode=predcls, type=10 Shot Recall.
 zR @ 20: 0.1135;  zR @ 50: 0.2116;  zR @ 100: 0.2648;  for mode=predcls, type=20 Shot Recall.
 zR @ 20: 0.1399;  zR @ 50: 0.1851;  zR @ 100: 0.2095;  for mode=predcls, type=25 Shot Recall.
 zR @ 20: 0.0909;  zR @ 50: 0.1688;  zR @ 100: 0.1818;  for mode=predcls, type=30 Shot Recall.
 zR @ 20: 0.4419;  zR @ 50: 0.5058;  zR @ 100: 0.5814;  for mode=predcls, type=40 Shot Recall.
 zR @ 20: 0.1875;  zR @ 50: 0.4583;  zR @ 100: 0.5312;  for mode=predcls, type=50 Shot Recall.
 zR @ 20: 0.3718;  zR @ 50: 0.4615;  zR @ 100: 0.5000;  for mode=predcls, type=100 Shot Recall.
 zR @ 20: 0.6667;  zR @ 50: 0.6667;  zR @ 100: 0.6667;  for mode=predcls, type=200 Shot Recall.
SGG eval:  mR @ 20: 0.0467;  mR @ 50: 0.0564;  mR @ 100: 0.0628;  for mode=predcls, type=Mean Recall.
(above:0.0115) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0000) (attached to:0.0000) (behind:0.1110) (belonging to:0.0000) (between:0.0000) (carrying:0.0000) (covered in:0.0357) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0000) (has:0.5619) (holding:0.1823) (in:0.0883) (in front of:0.0598) (laying on:0.0000) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.0375) (of:0.0862) (on:0.9536) (on back of:0.0000) (over:0.0000) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.0179) (says:0.0000) (sitting on:0.0218) (standing on:0.0000) (to:0.0000) (under:0.0102) (using:0.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.0000) (wearing:0.9598) (wears:0.0000) (with:0.0000) 
SGG eval:   A @ 20: 0.6129;   A @ 50: 0.6171;   A @ 100: 0.6171;  for mode=predcls, type=TopK Accuracy.
====================================================================================================

2023-02-27 10:33:53,664 maskrcnn_benchmark INFO: Validation Result: 0.5679
2023-02-27 10:34:45,922 maskrcnn_benchmark INFO: eta: 13:27:12  iter: 2200  loss: 4.6720 (5.4653)  loss_rel: 0.6434 (1.4367)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2478 (0.8379)  data: 0.0028 (0.5707)  lr: 0.000250  max mem: 4656
2023-02-27 10:35:37,429 maskrcnn_benchmark INFO: eta: 12:37:58  iter: 2400  loss: 4.3771 (5.3995)  loss_rel: 0.3485 (1.3709)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2523 (0.7896)  data: 0.0029 (0.5234)  lr: 0.000250  max mem: 4656
2023-02-27 10:36:29,273 maskrcnn_benchmark INFO: eta: 11:56:19  iter: 2600  loss: 4.5039 (5.3532)  loss_rel: 0.4753 (1.3246)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2488 (0.7488)  data: 0.0029 (0.4834)  lr: 0.000250  max mem: 4656
2023-02-27 10:37:22,013 maskrcnn_benchmark INFO: eta: 11:20:47  iter: 2800  loss: 4.4297 (5.3026)  loss_rel: 0.4011 (1.2740)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2492 (0.7141)  data: 0.0028 (0.4490)  lr: 0.000250  max mem: 4656
2023-02-27 10:38:13,810 maskrcnn_benchmark INFO: eta: 10:49:35  iter: 3000  loss: 4.3785 (5.2616)  loss_rel: 0.3499 (1.2330)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2554 (0.6838)  data: 0.0028 (0.4193)  lr: 0.000250  max mem: 4656
2023-02-27 10:39:06,117 maskrcnn_benchmark INFO: eta: 10:22:19  iter: 3200  loss: 4.3468 (5.2211)  loss_rel: 0.3182 (1.1925)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2402 (0.6574)  data: 0.0029 (0.3933)  lr: 0.000250  max mem: 4656
2023-02-27 10:39:58,829 maskrcnn_benchmark INFO: eta: 9:58:16  iter: 3400  loss: 4.3968 (5.1841)  loss_rel: 0.3682 (1.1555)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2590 (0.6342)  data: 0.0028 (0.3703)  lr: 0.000250  max mem: 4656
2023-02-27 10:40:51,652 maskrcnn_benchmark INFO: eta: 9:36:50  iter: 3600  loss: 4.4169 (5.1526)  loss_rel: 0.3883 (1.1240)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2495 (0.6137)  data: 0.0027 (0.3499)  lr: 0.000250  max mem: 4656
2023-02-27 10:41:44,469 maskrcnn_benchmark INFO: eta: 9:17:33  iter: 3800  loss: 4.3620 (5.1212)  loss_rel: 0.3334 (1.0926)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2441 (0.5953)  data: 0.0027 (0.3316)  lr: 0.000250  max mem: 4656
2023-02-27 10:42:37,450 maskrcnn_benchmark INFO: ---Total norm 23.41097 clip coef 0.21358-----------------
2023-02-27 10:42:37,463 maskrcnn_benchmark INFO: roi_heads.relation.predictor.ctx_compress.weight  : 13.28919, (torch.Size([51, 1024]))
2023-02-27 10:42:37,463 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_2.weight: 9.22620, (torch.Size([512, 2048, 1]))
2023-02-27 10:42:37,463 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.lin_edge.weight: 7.60133, (torch.Size([512, 4808]))
2023-02-27 10:42:37,463 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 6.69988, (torch.Size([4096, 12544]))
2023-02-27 10:42:37,463 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.fc.weight: 6.51666, (torch.Size([512, 512]))
2023-02-27 10:42:37,463 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 4.65744, (torch.Size([4096, 4096]))
2023-02-27 10:42:37,463 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_2.weight: 4.63438, (torch.Size([512, 2048, 1]))
2023-02-27 10:42:37,464 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_vs.weight: 4.09399, (torch.Size([512, 512]))
2023-02-27 10:42:37,464 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_1.weight: 4.05074, (torch.Size([2048, 512, 1]))
2023-02-27 10:42:37,464 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.fc.weight: 3.35371, (torch.Size([512, 512]))
2023-02-27 10:42:37,464 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_cat.weight      : 3.25905, (torch.Size([4096, 1024]))
2023-02-27 10:42:37,464 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc7.weight: 2.70233, (torch.Size([4096, 4096]))
2023-02-27 10:42:37,464 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc6.weight: 2.42014, (torch.Size([4096, 12544]))
2023-02-27 10:42:37,464 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_1.weight: 2.10159, (torch.Size([2048, 512, 1]))
2023-02-27 10:42:37,464 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_vs.weight: 2.05610, (torch.Size([512, 512]))
2023-02-27 10:42:37,464 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 2.04050, (torch.Size([256, 1024, 3, 3]))
2023-02-27 10:42:37,464 maskrcnn_benchmark INFO: roi_heads.relation.predictor.rel_compress.weight  : 1.86013, (torch.Size([51, 4096]))
2023-02-27 10:42:37,465 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.lin_obj.weight: 1.81701, (torch.Size([512, 4424]))
2023-02-27 10:42:37,465 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 1.50268, (torch.Size([256, 128, 3, 3]))
2023-02-27 10:42:37,465 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_emb.weight      : 1.38000, (torch.Size([1024, 512]))
2023-02-27 10:42:37,465 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.weight: 0.82415, (torch.Size([512, 2048, 1]))
2023-02-27 10:42:37,465 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.weight: 0.74767, (torch.Size([512, 512]))
2023-02-27 10:42:37,465 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.weight: 0.68409, (torch.Size([512, 2048, 1]))
2023-02-27 10:42:37,465 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.layer_norm.bias: 0.61862, (torch.Size([512]))
2023-02-27 10:42:37,465 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.layer_norm.weight: 0.57447, (torch.Size([512]))
2023-02-27 10:42:37,465 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.weight: 0.56568, (torch.Size([512, 512]))
2023-02-27 10:42:37,465 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.53625, (torch.Size([128, 2, 7, 7]))
2023-02-27 10:42:37,465 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.weight: 0.50510, (torch.Size([512, 512]))
2023-02-27 10:42:37,466 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.weight: 0.49834, (torch.Size([512, 2048, 1]))
2023-02-27 10:42:37,466 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_2.bias: 0.47979, (torch.Size([512]))
2023-02-27 10:42:37,466 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.layer_norm.bias: 0.46595, (torch.Size([512]))
2023-02-27 10:42:37,466 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.weight: 0.44973, (torch.Size([512, 2048, 1]))
2023-02-27 10:42:37,466 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.weight: 0.44282, (torch.Size([512, 512]))
2023-02-27 10:42:37,466 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.layer_norm.weight: 0.41722, (torch.Size([512]))
2023-02-27 10:42:37,466 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.weight: 0.39379, (torch.Size([2048, 512, 1]))
2023-02-27 10:42:37,466 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.weight: 0.37873, (torch.Size([512, 512]))
2023-02-27 10:42:37,466 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.weight: 0.37797, (torch.Size([512, 512]))
2023-02-27 10:42:37,466 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.weight: 0.31905, (torch.Size([2048, 512, 1]))
2023-02-27 10:42:37,466 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.weight: 0.31044, (torch.Size([512, 512]))
2023-02-27 10:42:37,467 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.layer_norm.bias: 0.29004, (torch.Size([512]))
2023-02-27 10:42:37,467 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.lin_edge.bias: 0.28312, (torch.Size([512]))
2023-02-27 10:42:37,467 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.layer_norm.weight: 0.24904, (torch.Size([512]))
2023-02-27 10:42:37,467 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_2.bias: 0.24789, (torch.Size([512]))
2023-02-27 10:42:37,467 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.layer_norm.bias: 0.24662, (torch.Size([512]))
2023-02-27 10:42:37,467 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.weight: 0.24037, (torch.Size([512, 512]))
2023-02-27 10:42:37,467 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.weight: 0.22449, (torch.Size([2048, 512, 1]))
2023-02-27 10:42:37,467 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_qs.weight: 0.22281, (torch.Size([512, 512]))
2023-02-27 10:42:37,467 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.layer_norm.weight: 0.21864, (torch.Size([512]))
2023-02-27 10:42:37,467 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_ks.weight: 0.21835, (torch.Size([512, 512]))
2023-02-27 10:42:37,467 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.fc.bias: 0.19768, (torch.Size([512]))
2023-02-27 10:42:37,468 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.weight: 0.18929, (torch.Size([2048, 512, 1]))
2023-02-27 10:42:37,468 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_vs.bias: 0.18744, (torch.Size([512]))
2023-02-27 10:42:37,468 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.fc.bias: 0.18670, (torch.Size([512]))
2023-02-27 10:42:37,468 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_vs.bias: 0.18399, (torch.Size([512]))
2023-02-27 10:42:37,468 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_1.bias: 0.18228, (torch.Size([2048]))
2023-02-27 10:42:37,468 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.lin_obj.bias: 0.12320, (torch.Size([512]))
2023-02-27 10:42:37,468 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_1.bias: 0.09601, (torch.Size([2048]))
2023-02-27 10:42:37,468 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.bias: 0.07746, (torch.Size([512]))
2023-02-27 10:42:37,468 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.07501, (torch.Size([128]))
2023-02-27 10:42:37,468 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.bias: 0.07448, (torch.Size([512]))
2023-02-27 10:42:37,469 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.weight: 0.06444, (torch.Size([512, 512]))
2023-02-27 10:42:37,469 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_emb.bias        : 0.06212, (torch.Size([1024]))
2023-02-27 10:42:37,469 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.weight: 0.06044, (torch.Size([512, 512]))
2023-02-27 10:42:37,469 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_qs.weight: 0.05877, (torch.Size([512, 512]))
2023-02-27 10:42:37,469 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_ks.weight: 0.05456, (torch.Size([512, 512]))
2023-02-27 10:42:37,469 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.bias: 0.04982, (torch.Size([512]))
2023-02-27 10:42:37,469 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.weight: 0.04931, (torch.Size([512, 512]))
2023-02-27 10:42:37,469 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.weight: 0.04866, (torch.Size([512, 512]))
2023-02-27 10:42:37,469 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.bias: 0.04817, (torch.Size([512]))
2023-02-27 10:42:37,469 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.bias: 0.04715, (torch.Size([512]))
2023-02-27 10:42:37,469 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.weight: 0.04704, (torch.Size([512]))
2023-02-27 10:42:37,470 maskrcnn_benchmark INFO: roi_heads.relation.predictor.rel_compress.bias    : 0.04589, (torch.Size([51]))
2023-02-27 10:42:37,470 maskrcnn_benchmark INFO: roi_heads.relation.predictor.ctx_compress.bias    : 0.04589, (torch.Size([51]))
2023-02-27 10:42:37,470 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.weight: 0.04564, (torch.Size([512]))
2023-02-27 10:42:37,470 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.04381, (torch.Size([4096]))
2023-02-27 10:42:37,470 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.weight: 0.04356, (torch.Size([512, 512]))
2023-02-27 10:42:37,470 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc7.bias : 0.04276, (torch.Size([4096]))
2023-02-27 10:42:37,470 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.weight: 0.04200, (torch.Size([512, 512]))
2023-02-27 10:42:37,470 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.bias: 0.03946, (torch.Size([512]))
2023-02-27 10:42:37,470 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.weight: 0.03918, (torch.Size([512]))
2023-02-27 10:42:37,470 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.bias: 0.03747, (torch.Size([512]))
2023-02-27 10:42:37,471 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.bias: 0.03716, (torch.Size([512]))
2023-02-27 10:42:37,471 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.weight: 0.03692, (torch.Size([512]))
2023-02-27 10:42:37,471 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.03585, (torch.Size([256]))
2023-02-27 10:42:37,471 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.bias: 0.03083, (torch.Size([512]))
2023-02-27 10:42:37,471 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.bias: 0.03020, (torch.Size([512]))
2023-02-27 10:42:37,471 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.bias: 0.02967, (torch.Size([512]))
2023-02-27 10:42:37,471 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.bias: 0.02921, (torch.Size([512]))
2023-02-27 10:42:37,471 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.bias: 0.02819, (torch.Size([512]))
2023-02-27 10:42:37,471 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.bias: 0.02811, (torch.Size([512]))
2023-02-27 10:42:37,471 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.weight: 0.02761, (torch.Size([512]))
2023-02-27 10:42:37,471 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.weight: 0.02679, (torch.Size([512]))
2023-02-27 10:42:37,472 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.weight: 0.02629, (torch.Size([512]))
2023-02-27 10:42:37,472 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.weight: 0.02538, (torch.Size([512]))
2023-02-27 10:42:37,472 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.02518, (torch.Size([128]))
2023-02-27 10:42:37,472 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.bias: 0.02461, (torch.Size([512]))
2023-02-27 10:42:37,472 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.02419, (torch.Size([256]))
2023-02-27 10:42:37,472 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.bias: 0.02365, (torch.Size([512]))
2023-02-27 10:42:37,472 maskrcnn_benchmark INFO: roi_heads.relation.predictor.freq_bias.obj_baseline.weight: 0.02215, (torch.Size([22801, 51]))
2023-02-27 10:42:37,472 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.02164, (torch.Size([4096]))
2023-02-27 10:42:37,472 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.01995, (torch.Size([256]))
2023-02-27 10:42:37,472 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.bias: 0.01961, (torch.Size([512]))
2023-02-27 10:42:37,472 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.bias: 0.01860, (torch.Size([512]))
2023-02-27 10:42:37,473 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.bias: 0.01821, (torch.Size([2048]))
2023-02-27 10:42:37,473 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.bias: 0.01743, (torch.Size([512]))
2023-02-27 10:42:37,473 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.bias: 0.01680, (torch.Size([512]))
2023-02-27 10:42:37,473 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.01590, (torch.Size([256]))
2023-02-27 10:42:37,473 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc6.bias : 0.01515, (torch.Size([4096]))
2023-02-27 10:42:37,473 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.bias: 0.01486, (torch.Size([2048]))
2023-02-27 10:42:37,473 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.01290, (torch.Size([151, 200]))
2023-02-27 10:42:37,473 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_cat.bias        : 0.01122, (torch.Size([4096]))
2023-02-27 10:42:37,473 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.bias: 0.01085, (torch.Size([2048]))
2023-02-27 10:42:37,473 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_qs.bias: 0.01002, (torch.Size([512]))
2023-02-27 10:42:37,473 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.bias: 0.00985, (torch.Size([2048]))
2023-02-27 10:42:37,474 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.00958, (torch.Size([128]))
2023-02-27 10:42:37,474 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00753, (torch.Size([128]))
2023-02-27 10:42:37,474 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.weight: 0.00745, (torch.Size([512, 512]))
2023-02-27 10:42:37,474 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.weight: 0.00736, (torch.Size([512, 512]))
2023-02-27 10:42:37,474 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.00614, (torch.Size([151, 200]))
2023-02-27 10:42:37,474 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_qs.bias: 0.00498, (torch.Size([512]))
2023-02-27 10:42:37,474 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00392, (torch.Size([128, 32]))
2023-02-27 10:42:37,474 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.bias: 0.00330, (torch.Size([512]))
2023-02-27 10:42:37,474 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00256, (torch.Size([32, 9]))
2023-02-27 10:42:37,474 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.bias: 0.00232, (torch.Size([512]))
2023-02-27 10:42:37,475 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00220, (torch.Size([32]))
2023-02-27 10:42:37,475 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.bias: 0.00191, (torch.Size([512]))
2023-02-27 10:42:37,475 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.bias: 0.00181, (torch.Size([512]))
2023-02-27 10:42:37,475 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2023-02-27 10:42:37,475 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2023-02-27 10:42:37,475 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2023-02-27 10:42:37,475 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2023-02-27 10:42:37,475 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2023-02-27 10:42:37,475 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2023-02-27 10:42:37,475 maskrcnn_benchmark INFO: -------------------------------
2023-02-27 10:42:37,479 maskrcnn_benchmark INFO: eta: 9:00:09  iter: 4000  loss: 4.2544 (5.0886)  loss_rel: 0.2258 (1.0600)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2573 (0.5787)  data: 0.0027 (0.3152)  lr: 0.000250  max mem: 4656
2023-02-27 10:42:37,482 maskrcnn_benchmark.utils.checkpoint INFO: Saving checkpoint to /users/students/r0879687/amager/vg/output/relation_baseline/model_0004000.pth
2023-02-27 10:43:18,934 maskrcnn_benchmark INFO: Start validating
2023-02-27 10:43:18,944 maskrcnn_benchmark INFO: Start evaluation on VG_stanford_filtered_with_attribute_val dataset(5000 images).
2023-02-27 10:59:34,761 maskrcnn_benchmark INFO: Total run time: 0:16:15.816571 (0.19516331429481507 s / img per device, on 1 devices)
2023-02-27 10:59:34,762 maskrcnn_benchmark INFO: Model inference time: 0:15:52.612009 (0.19052240180969238 s / img per device, on 1 devices)
creating index...
index created!
Loading and preparing results...
Converting ndarray to lists...
(62754, 7)
0/62754
DONE (t=0.89s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=26.82s).
Accumulating evaluation results...
DONE (t=3.78s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.675
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.994
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000
2023-02-27 11:03:30,696 maskrcnn_benchmark INFO: 
====================================================================================================
Detection evaluation mAp=0.9999
====================================================================================================
SGG eval:   R @ 20: 0.5308;   R @ 50: 0.5932;   R @ 100: 0.6175;  for mode=predcls, type=Recall(Main).
SGG eval: ngR @ 20: 0.5870; ngR @ 50: 0.7103; ngR @ 100: 0.7882;  for mode=predcls, type=No Graph Constraint Recall(Main).
SGG eval:  zR @ 20: 0.0089;  zR @ 50: 0.0511;  zR @ 100: 0.0667;  for mode=predcls, type=Zero Shot Recall.
SGG eval:  zR @ 20: 0.0659;  zR @ 50: 0.1052;  zR @ 100: 0.1424;  for mode=predcls, type=1 Shot Recall.
 zR @ 20: 0.0781;  zR @ 50: 0.1336;  zR @ 100: 0.1592;  for mode=predcls, type=2 Shot Recall.
 zR @ 20: 0.0996;  zR @ 50: 0.1530;  zR @ 100: 0.1919;  for mode=predcls, type=3 Shot Recall.
 zR @ 20: 0.1165;  zR @ 50: 0.1803;  zR @ 100: 0.2350;  for mode=predcls, type=4 Shot Recall.
 zR @ 20: 0.1524;  zR @ 50: 0.2127;  zR @ 100: 0.2469;  for mode=predcls, type=5 Shot Recall.
 zR @ 20: 0.1872;  zR @ 50: 0.2513;  zR @ 100: 0.2940;  for mode=predcls, type=6 Shot Recall.
 zR @ 20: 0.1577;  zR @ 50: 0.2000;  zR @ 100: 0.2528;  for mode=predcls, type=7 Shot Recall.
 zR @ 20: 0.1394;  zR @ 50: 0.1939;  zR @ 100: 0.2273;  for mode=predcls, type=8 Shot Recall.
 zR @ 20: 0.2410;  zR @ 50: 0.3381;  zR @ 100: 0.3633;  for mode=predcls, type=9 Shot Recall.
 zR @ 20: 0.1867;  zR @ 50: 0.2444;  zR @ 100: 0.2894;  for mode=predcls, type=10 Shot Recall.
 zR @ 20: 0.1495;  zR @ 50: 0.2470;  zR @ 100: 0.3322;  for mode=predcls, type=20 Shot Recall.
 zR @ 20: 0.1518;  zR @ 50: 0.2458;  zR @ 100: 0.3095;  for mode=predcls, type=25 Shot Recall.
 zR @ 20: 0.1429;  zR @ 50: 0.1688;  zR @ 100: 0.2078;  for mode=predcls, type=30 Shot Recall.
 zR @ 20: 0.4302;  zR @ 50: 0.5407;  zR @ 100: 0.5581;  for mode=predcls, type=40 Shot Recall.
 zR @ 20: 0.3452;  zR @ 50: 0.5833;  zR @ 100: 0.5833;  for mode=predcls, type=50 Shot Recall.
 zR @ 20: 0.5000;  zR @ 50: 0.5769;  zR @ 100: 0.5769;  for mode=predcls, type=100 Shot Recall.
 zR @ 20: 0.3333;  zR @ 50: 0.6667;  zR @ 100: 1.0000;  for mode=predcls, type=200 Shot Recall.
SGG eval:  mR @ 20: 0.0697;  mR @ 50: 0.0842;  mR @ 100: 0.0918;  for mode=predcls, type=Mean Recall.
(above:0.0172) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0000) (attached to:0.0000) (behind:0.2868) (belonging to:0.0000) (between:0.0000) (carrying:0.0000) (covered in:0.0357) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0000) (has:0.6799) (holding:0.6676) (in:0.3995) (in front of:0.0045) (laying on:0.0000) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.2116) (of:0.1920) (on:0.9105) (on back of:0.0000) (over:0.0000) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.0089) (says:0.0000) (sitting on:0.0087) (standing on:0.0109) (to:0.0000) (under:0.0459) (using:0.0000) (walking in:0.0000) (walking on:0.0160) (watching:0.0000) (wearing:0.9472) (wears:0.0000) (with:0.1494) 
SGG eval:   A @ 20: 0.6508;   A @ 50: 0.6551;   A @ 100: 0.6551;  for mode=predcls, type=TopK Accuracy.
====================================================================================================

2023-02-27 11:03:31,075 maskrcnn_benchmark INFO: Validation Result: 0.6175
2023-02-27 11:04:23,593 maskrcnn_benchmark INFO: eta: 13:21:48  iter: 4200  loss: 4.4264 (5.0624)  loss_rel: 0.3978 (1.0338)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2349 (0.8622)  data: 0.0028 (0.5988)  lr: 0.000250  max mem: 4656
2023-02-27 11:05:15,707 maskrcnn_benchmark INFO: eta: 12:53:36  iter: 4400  loss: 4.3919 (5.0368)  loss_rel: 0.3633 (1.0082)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2597 (0.8348)  data: 0.0028 (0.5717)  lr: 0.000250  max mem: 4656
2023-02-27 11:06:08,464 maskrcnn_benchmark INFO: eta: 12:27:53  iter: 4600  loss: 4.3950 (5.0170)  loss_rel: 0.3664 (0.9884)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2539 (0.8100)  data: 0.0028 (0.5470)  lr: 0.000250  max mem: 4656
2023-02-27 11:07:00,876 maskrcnn_benchmark INFO: eta: 12:04:11  iter: 4800  loss: 4.5056 (5.0015)  loss_rel: 0.4770 (0.9730)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2495 (0.7872)  data: 0.0028 (0.5243)  lr: 0.000250  max mem: 4656
2023-02-27 11:07:52,413 maskrcnn_benchmark INFO: eta: 11:42:09  iter: 5000  loss: 4.4103 (4.9812)  loss_rel: 0.3817 (0.9526)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2560 (0.7660)  data: 0.0028 (0.5034)  lr: 0.000250  max mem: 4656
2023-02-27 11:08:45,837 maskrcnn_benchmark INFO: eta: 11:22:04  iter: 5200  loss: 4.2990 (4.9632)  loss_rel: 0.2704 (0.9346)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2565 (0.7468)  data: 0.0028 (0.4842)  lr: 0.000250  max mem: 4656
2023-02-27 11:09:40,105 maskrcnn_benchmark INFO: eta: 11:03:33  iter: 5400  loss: 4.2005 (4.9449)  loss_rel: 0.1719 (0.9163)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2512 (0.7292)  data: 0.0029 (0.4663)  lr: 0.000250  max mem: 4656
2023-02-27 11:10:33,530 maskrcnn_benchmark INFO: eta: 10:46:10  iter: 5600  loss: 4.3531 (4.9286)  loss_rel: 0.3245 (0.9000)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2557 (0.7127)  data: 0.0029 (0.4498)  lr: 0.000250  max mem: 4656
2023-02-27 11:11:30,382 maskrcnn_benchmark INFO: eta: 10:30:26  iter: 5800  loss: 4.3435 (4.9154)  loss_rel: 0.3149 (0.8868)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2872 (0.6979)  data: 0.0045 (0.4344)  lr: 0.000250  max mem: 4656
2023-02-27 11:12:29,883 maskrcnn_benchmark INFO: eta: 10:16:06  iter: 6000  loss: 4.3959 (4.9010)  loss_rel: 0.3673 (0.8724)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2893 (0.6846)  data: 0.0046 (0.4201)  lr: 0.000250  max mem: 4656
2023-02-27 11:12:29,889 maskrcnn_benchmark.utils.checkpoint INFO: Saving checkpoint to /users/students/r0879687/amager/vg/output/relation_baseline/model_0006000.pth
2023-02-27 11:13:10,985 maskrcnn_benchmark INFO: Start validating
2023-02-27 11:13:10,990 maskrcnn_benchmark INFO: Start evaluation on VG_stanford_filtered_with_attribute_val dataset(5000 images).
2023-02-27 11:30:32,337 maskrcnn_benchmark INFO: Total run time: 0:17:21.346698 (0.20826933965682984 s / img per device, on 1 devices)
2023-02-27 11:30:32,338 maskrcnn_benchmark INFO: Model inference time: 0:16:51.303741 (0.20226074810028077 s / img per device, on 1 devices)
creating index...
index created!
Loading and preparing results...
Converting ndarray to lists...
(62754, 7)
0/62754
DONE (t=0.58s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=29.33s).
Accumulating evaluation results...
DONE (t=8.01s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.675
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.994
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000
2023-02-27 11:35:50,716 maskrcnn_benchmark INFO: 
====================================================================================================
Detection evaluation mAp=0.9999
====================================================================================================
SGG eval:   R @ 20: 0.5311;   R @ 50: 0.5887;   R @ 100: 0.6103;  for mode=predcls, type=Recall(Main).
SGG eval: ngR @ 20: 0.5879; ngR @ 50: 0.7084; ngR @ 100: 0.7881;  for mode=predcls, type=No Graph Constraint Recall(Main).
SGG eval:  zR @ 20: 0.0000;  zR @ 50: 0.0467;  zR @ 100: 0.0756;  for mode=predcls, type=Zero Shot Recall.
SGG eval:  zR @ 20: 0.0650;  zR @ 50: 0.1057;  zR @ 100: 0.1272;  for mode=predcls, type=1 Shot Recall.
 zR @ 20: 0.0721;  zR @ 50: 0.1126;  zR @ 100: 0.1396;  for mode=predcls, type=2 Shot Recall.
 zR @ 20: 0.0788;  zR @ 50: 0.1014;  zR @ 100: 0.1451;  for mode=predcls, type=3 Shot Recall.
 zR @ 20: 0.1131;  zR @ 50: 0.1749;  zR @ 100: 0.2157;  for mode=predcls, type=4 Shot Recall.
 zR @ 20: 0.1332;  zR @ 50: 0.1881;  zR @ 100: 0.2102;  for mode=predcls, type=5 Shot Recall.
 zR @ 20: 0.1641;  zR @ 50: 0.2410;  zR @ 100: 0.2615;  for mode=predcls, type=6 Shot Recall.
 zR @ 20: 0.0991;  zR @ 50: 0.1488;  zR @ 100: 0.1793;  for mode=predcls, type=7 Shot Recall.
 zR @ 20: 0.1455;  zR @ 50: 0.1903;  zR @ 100: 0.2182;  for mode=predcls, type=8 Shot Recall.
 zR @ 20: 0.2558;  zR @ 50: 0.3507;  zR @ 100: 0.3633;  for mode=predcls, type=9 Shot Recall.
 zR @ 20: 0.1577;  zR @ 50: 0.1982;  zR @ 100: 0.2320;  for mode=predcls, type=10 Shot Recall.
 zR @ 20: 0.1797;  zR @ 50: 0.2595;  zR @ 100: 0.3073;  for mode=predcls, type=20 Shot Recall.
 zR @ 20: 0.1732;  zR @ 50: 0.2286;  zR @ 100: 0.2750;  for mode=predcls, type=25 Shot Recall.
 zR @ 20: 0.1299;  zR @ 50: 0.1818;  zR @ 100: 0.2273;  for mode=predcls, type=30 Shot Recall.
 zR @ 20: 0.5000;  zR @ 50: 0.5581;  zR @ 100: 0.5698;  for mode=predcls, type=40 Shot Recall.
 zR @ 20: 0.3244;  zR @ 50: 0.4792;  zR @ 100: 0.5677;  for mode=predcls, type=50 Shot Recall.
 zR @ 20: 0.5000;  zR @ 50: 0.5769;  zR @ 100: 0.5769;  for mode=predcls, type=100 Shot Recall.
 zR @ 20: 0.6667;  zR @ 50: 1.0000;  zR @ 100: 1.0000;  for mode=predcls, type=200 Shot Recall.
SGG eval:  mR @ 20: 0.0537;  mR @ 50: 0.0657;  mR @ 100: 0.0727;  for mode=predcls, type=Mean Recall.
(above:0.0458) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0000) (attached to:0.0000) (behind:0.0838) (belonging to:0.0000) (between:0.0000) (carrying:0.0263) (covered in:0.0357) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0000) (has:0.6914) (holding:0.0880) (in:0.2880) (in front of:0.0716) (laying on:0.0000) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.1179) (of:0.2106) (on:0.9602) (on back of:0.0000) (over:0.0000) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.0000) (says:0.0000) (sitting on:0.0000) (standing on:0.0000) (to:0.0000) (under:0.0204) (using:0.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.0000) (wearing:0.9726) (wears:0.0000) (with:0.0202) 
SGG eval:   A @ 20: 0.6474;   A @ 50: 0.6514;   A @ 100: 0.6514;  for mode=predcls, type=TopK Accuracy.
====================================================================================================

2023-02-27 11:35:51,103 maskrcnn_benchmark INFO: Validation Result: 0.6103
2023-02-27 11:36:42,950 maskrcnn_benchmark INFO: eta: 13:24:10  iter: 6200  loss: 4.2186 (4.8891)  loss_rel: 0.1900 (0.8605)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2577 (0.8968)  data: 0.0029 (0.6326)  lr: 0.000250  max mem: 4656
2023-02-27 11:37:34,590 maskrcnn_benchmark INFO: eta: 13:03:21  iter: 6400  loss: 4.3242 (4.8773)  loss_rel: 0.2956 (0.8487)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2529 (0.8769)  data: 0.0029 (0.6130)  lr: 0.000250  max mem: 4656
2023-02-27 11:38:26,052 maskrcnn_benchmark INFO: eta: 12:43:43  iter: 6600  loss: 4.2616 (4.8652)  loss_rel: 0.2330 (0.8366)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2543 (0.8581)  data: 0.0029 (0.5945)  lr: 0.000250  max mem: 4656
2023-02-27 11:39:18,034 maskrcnn_benchmark INFO: eta: 12:25:15  iter: 6800  loss: 4.3391 (4.8545)  loss_rel: 0.3105 (0.8259)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2464 (0.8405)  data: 0.0029 (0.5771)  lr: 0.000250  max mem: 4656
2023-02-27 11:40:10,083 maskrcnn_benchmark INFO: eta: 12:07:48  iter: 7000  loss: 4.1948 (4.8468)  loss_rel: 0.1662 (0.8183)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2534 (0.8239)  data: 0.0027 (0.5607)  lr: 0.000250  max mem: 4656
2023-02-27 11:41:01,827 maskrcnn_benchmark INFO: eta: 11:51:15  iter: 7200  loss: 4.4019 (4.8364)  loss_rel: 0.3733 (0.8078)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2356 (0.8082)  data: 0.0028 (0.5452)  lr: 0.000250  max mem: 4656
2023-02-27 11:41:54,472 maskrcnn_benchmark INFO: eta: 11:35:38  iter: 7400  loss: 4.2271 (4.8257)  loss_rel: 0.1985 (0.7971)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2571 (0.7935)  data: 0.0029 (0.5305)  lr: 0.000250  max mem: 4656
2023-02-27 11:42:46,009 maskrcnn_benchmark INFO: eta: 11:20:41  iter: 7600  loss: 4.2677 (4.8164)  loss_rel: 0.2391 (0.7879)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2510 (0.7794)  data: 0.0028 (0.5166)  lr: 0.000250  max mem: 4656
2023-02-27 11:43:37,921 maskrcnn_benchmark INFO: eta: 11:06:29  iter: 7800  loss: 4.2912 (4.8072)  loss_rel: 0.2626 (0.7786)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2553 (0.7661)  data: 0.0028 (0.5035)  lr: 0.000250  max mem: 4656
2023-02-27 11:44:30,872 maskrcnn_benchmark INFO: ---Total norm 6.66815 clip coef 0.74983-----------------
2023-02-27 11:44:30,885 maskrcnn_benchmark INFO: roi_heads.relation.predictor.ctx_compress.weight  : 4.41373, (torch.Size([51, 1024]))
2023-02-27 11:44:30,885 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_2.weight: 2.49125, (torch.Size([512, 2048, 1]))
2023-02-27 11:44:30,885 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.weight: 1.89239, (torch.Size([4096, 12544]))
2023-02-27 11:44:30,885 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.lin_edge.weight: 1.54655, (torch.Size([512, 4808]))
2023-02-27 11:44:30,885 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.fc.weight: 1.54644, (torch.Size([512, 512]))
2023-02-27 11:44:30,885 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.weight: 1.40783, (torch.Size([4096, 4096]))
2023-02-27 11:44:30,885 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_cat.weight      : 1.30003, (torch.Size([4096, 1024]))
2023-02-27 11:44:30,885 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_1.weight: 1.13757, (torch.Size([2048, 512, 1]))
2023-02-27 11:44:30,886 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_2.weight: 1.13550, (torch.Size([512, 2048, 1]))
2023-02-27 11:44:30,886 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_vs.weight: 1.00444, (torch.Size([512, 512]))
2023-02-27 11:44:30,886 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.fc.weight: 0.78183, (torch.Size([512, 512]))
2023-02-27 11:44:30,886 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.weight: 0.66216, (torch.Size([256, 1024, 3, 3]))
2023-02-27 11:44:30,886 maskrcnn_benchmark INFO: roi_heads.relation.predictor.rel_compress.weight  : 0.54085, (torch.Size([51, 4096]))
2023-02-27 11:44:30,886 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc7.weight: 0.51719, (torch.Size([4096, 4096]))
2023-02-27 11:44:30,886 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_vs.weight: 0.50472, (torch.Size([512, 512]))
2023-02-27 11:44:30,886 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_1.weight: 0.49507, (torch.Size([2048, 512, 1]))
2023-02-27 11:44:30,886 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc6.weight: 0.47554, (torch.Size([4096, 12544]))
2023-02-27 11:44:30,886 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_emb.weight      : 0.44975, (torch.Size([1024, 512]))
2023-02-27 11:44:30,887 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.4.weight: 0.40794, (torch.Size([256, 128, 3, 3]))
2023-02-27 11:44:30,887 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.lin_obj.weight: 0.35859, (torch.Size([512, 4424]))
2023-02-27 11:44:30,887 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.layer_norm.bias: 0.17866, (torch.Size([512]))
2023-02-27 11:44:30,887 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.weight: 0.17233, (torch.Size([512, 2048, 1]))
2023-02-27 11:44:30,887 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.weight: 0.15690, (torch.Size([512, 512]))
2023-02-27 11:44:30,887 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.weight: 0.15000, (torch.Size([512, 2048, 1]))
2023-02-27 11:44:30,887 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.0.weight: 0.13430, (torch.Size([128, 2, 7, 7]))
2023-02-27 11:44:30,887 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.layer_norm.weight: 0.13037, (torch.Size([512]))
2023-02-27 11:44:30,887 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_2.bias: 0.12758, (torch.Size([512]))
2023-02-27 11:44:30,887 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.weight: 0.12480, (torch.Size([512, 512]))
2023-02-27 11:44:30,887 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.layer_norm.bias: 0.11692, (torch.Size([512]))
2023-02-27 11:44:30,888 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.weight: 0.11353, (torch.Size([512, 2048, 1]))
2023-02-27 11:44:30,888 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.weight: 0.10527, (torch.Size([512, 512]))
2023-02-27 11:44:30,888 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.weight: 0.09841, (torch.Size([512, 512]))
2023-02-27 11:44:30,888 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.weight: 0.09094, (torch.Size([512, 2048, 1]))
2023-02-27 11:44:30,888 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.weight: 0.08730, (torch.Size([512, 512]))
2023-02-27 11:44:30,888 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.layer_norm.weight: 0.08193, (torch.Size([512]))
2023-02-27 11:44:30,888 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.weight: 0.08161, (torch.Size([2048, 512, 1]))
2023-02-27 11:44:30,888 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.weight: 0.07477, (torch.Size([512, 512]))
2023-02-27 11:44:30,888 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.layer_norm.bias: 0.07312, (torch.Size([512]))
2023-02-27 11:44:30,888 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.weight: 0.06927, (torch.Size([2048, 512, 1]))
2023-02-27 11:44:30,889 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.weight: 0.06776, (torch.Size([512, 512]))
2023-02-27 11:44:30,889 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.layer_norm.weight: 0.06493, (torch.Size([512]))
2023-02-27 11:44:30,889 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_2.bias: 0.05965, (torch.Size([512]))
2023-02-27 11:44:30,889 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.lin_edge.bias: 0.05861, (torch.Size([512]))
2023-02-27 11:44:30,889 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.layer_norm.bias: 0.05781, (torch.Size([512]))
2023-02-27 11:44:30,889 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.layer_norm.weight: 0.05112, (torch.Size([512]))
2023-02-27 11:44:30,889 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.pos_ffn.w_1.bias: 0.05072, (torch.Size([2048]))
2023-02-27 11:44:30,889 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.weight: 0.04951, (torch.Size([512, 512]))
2023-02-27 11:44:30,889 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.weight: 0.04924, (torch.Size([2048, 512, 1]))
2023-02-27 11:44:30,889 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.fc.bias: 0.04844, (torch.Size([512]))
2023-02-27 11:44:30,889 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_vs.bias: 0.04563, (torch.Size([512]))
2023-02-27 11:44:30,890 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_vs.bias: 0.04020, (torch.Size([512]))
2023-02-27 11:44:30,890 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.fc.bias: 0.03947, (torch.Size([512]))
2023-02-27 11:44:30,890 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.weight: 0.03882, (torch.Size([2048, 512, 1]))
2023-02-27 11:44:30,890 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_ks.weight: 0.03670, (torch.Size([512, 512]))
2023-02-27 11:44:30,890 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_qs.weight: 0.03523, (torch.Size([512, 512]))
2023-02-27 11:44:30,890 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.lin_obj.bias: 0.02603, (torch.Size([512]))
2023-02-27 11:44:30,890 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.pos_ffn.w_1.bias: 0.02243, (torch.Size([2048]))
2023-02-27 11:44:30,890 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_emb.bias        : 0.02022, (torch.Size([1024]))
2023-02-27 11:44:30,890 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.0.bias: 0.02001, (torch.Size([128]))
2023-02-27 11:44:30,890 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.weight: 0.01805, (torch.Size([512, 512]))
2023-02-27 11:44:30,890 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.weight: 0.01651, (torch.Size([512, 512]))
2023-02-27 11:44:30,891 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_vs.bias: 0.01616, (torch.Size([512]))
2023-02-27 11:44:30,891 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.fc.bias: 0.01607, (torch.Size([512]))
2023-02-27 11:44:30,891 maskrcnn_benchmark INFO: roi_heads.relation.predictor.rel_compress.bias    : 0.01574, (torch.Size([51]))
2023-02-27 11:44:30,891 maskrcnn_benchmark INFO: roi_heads.relation.predictor.ctx_compress.bias    : 0.01574, (torch.Size([51]))
2023-02-27 11:44:30,891 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_qs.weight: 0.01512, (torch.Size([512, 512]))
2023-02-27 11:44:30,891 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_ks.weight: 0.01384, (torch.Size([512, 512]))
2023-02-27 11:44:30,891 maskrcnn_benchmark INFO: roi_heads.relation.predictor.freq_bias.obj_baseline.weight: 0.01319, (torch.Size([22801, 51]))
2023-02-27 11:44:30,891 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.bias: 0.01023, (torch.Size([512]))
2023-02-27 11:44:30,891 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.weight: 0.01016, (torch.Size([512, 512]))
2023-02-27 11:44:30,891 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc7.bias: 0.01000, (torch.Size([4096]))
2023-02-27 11:44:30,891 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.bias: 0.00973, (torch.Size([512]))
2023-02-27 11:44:30,892 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_2.bias: 0.00968, (torch.Size([512]))
2023-02-27 11:44:30,892 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.weight: 0.00950, (torch.Size([512, 512]))
2023-02-27 11:44:30,892 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.layer_norm.weight: 0.00933, (torch.Size([512]))
2023-02-27 11:44:30,892 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc7.bias : 0.00920, (torch.Size([4096]))
2023-02-27 11:44:30,892 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.layer_norm.weight: 0.00911, (torch.Size([512]))
2023-02-27 11:44:30,892 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.bias: 0.00852, (torch.Size([512]))
2023-02-27 11:44:30,892 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.6.bias: 0.00817, (torch.Size([256]))
2023-02-27 11:44:30,892 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.layer_norm.weight: 0.00814, (torch.Size([512]))
2023-02-27 11:44:30,892 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.bias: 0.00809, (torch.Size([512]))
2023-02-27 11:44:30,892 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_2.bias: 0.00798, (torch.Size([512]))
2023-02-27 11:44:30,892 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.layer_norm.weight: 0.00756, (torch.Size([512]))
2023-02-27 11:44:30,893 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.weight: 0.00693, (torch.Size([512]))
2023-02-27 11:44:30,893 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.layer_norm.bias: 0.00684, (torch.Size([512]))
2023-02-27 11:44:30,893 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.2.weight: 0.00682, (torch.Size([128]))
2023-02-27 11:44:30,893 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.weight: 0.00678, (torch.Size([512, 512]))
2023-02-27 11:44:30,893 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.weight: 0.00658, (torch.Size([512]))
2023-02-27 11:44:30,893 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.layer_norm.bias: 0.00653, (torch.Size([512]))
2023-02-27 11:44:30,893 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_2.bias: 0.00645, (torch.Size([512]))
2023-02-27 11:44:30,893 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.4.bias: 0.00642, (torch.Size([256]))
2023-02-27 11:44:30,893 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.weight: 0.00640, (torch.Size([512, 512]))
2023-02-27 11:44:30,893 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.6.weight: 0.00620, (torch.Size([256]))
2023-02-27 11:44:30,894 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.bias: 0.00586, (torch.Size([512]))
2023-02-27 11:44:30,894 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.fc6.bias: 0.00580, (torch.Size([4096]))
2023-02-27 11:44:30,894 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.feature_extractor.pooler.reduce_channel.0.bias: 0.00569, (torch.Size([256]))
2023-02-27 11:44:30,894 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.bias: 0.00565, (torch.Size([512]))
2023-02-27 11:44:30,894 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_2.bias: 0.00563, (torch.Size([512]))
2023-02-27 11:44:30,894 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.layer_norm.weight: 0.00545, (torch.Size([512]))
2023-02-27 11:44:30,894 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.layer_norm.weight: 0.00528, (torch.Size([512]))
2023-02-27 11:44:30,894 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.fc.bias: 0.00497, (torch.Size([512]))
2023-02-27 11:44:30,894 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_vs.bias: 0.00492, (torch.Size([512]))
2023-02-27 11:44:30,894 maskrcnn_benchmark INFO: roi_heads.relation.predictor.post_cat.bias        : 0.00465, (torch.Size([4096]))
2023-02-27 11:44:30,894 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.fc.bias: 0.00433, (torch.Size([512]))
2023-02-27 11:44:30,895 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_vs.bias: 0.00422, (torch.Size([512]))
2023-02-27 11:44:30,895 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.pos_ffn.w_1.bias: 0.00375, (torch.Size([2048]))
2023-02-27 11:44:30,895 maskrcnn_benchmark INFO: roi_heads.relation.union_feature_extractor.rect_conv.2.bias: 0.00372, (torch.Size([128]))
2023-02-27 11:44:30,895 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_embed2.weight: 0.00363, (torch.Size([151, 200]))
2023-02-27 11:44:30,895 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_vs.bias: 0.00361, (torch.Size([512]))
2023-02-27 11:44:30,895 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.fc.bias: 0.00357, (torch.Size([512]))
2023-02-27 11:44:30,895 maskrcnn_benchmark INFO: roi_heads.relation.box_feature_extractor.fc6.bias : 0.00325, (torch.Size([4096]))
2023-02-27 11:44:30,895 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.pos_ffn.w_1.bias: 0.00322, (torch.Size([2048]))
2023-02-27 11:44:30,895 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.pos_ffn.w_1.bias: 0.00236, (torch.Size([2048]))
2023-02-27 11:44:30,895 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.pos_ffn.w_1.bias: 0.00198, (torch.Size([2048]))
2023-02-27 11:44:30,895 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.bbox_embed.3.bias: 0.00169, (torch.Size([128]))
2023-02-27 11:44:30,896 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_qs.bias: 0.00156, (torch.Size([512]))
2023-02-27 11:44:30,896 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.obj_embed1.weight: 0.00145, (torch.Size([151, 200]))
2023-02-27 11:44:30,896 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_qs.bias: 0.00118, (torch.Size([512]))
2023-02-27 11:44:30,896 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.weight: 0.00115, (torch.Size([512, 512]))
2023-02-27 11:44:30,896 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.weight: 0.00110, (torch.Size([512, 512]))
2023-02-27 11:44:30,896 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_qs.bias: 0.00095, (torch.Size([512]))
2023-02-27 11:44:30,896 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.bbox_embed.3.weight: 0.00090, (torch.Size([128, 32]))
2023-02-27 11:44:30,896 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.bbox_embed.0.weight: 0.00077, (torch.Size([32, 9]))
2023-02-27 11:44:30,896 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.bbox_embed.0.bias: 0.00066, (torch.Size([32]))
2023-02-27 11:44:30,896 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_qs.bias: 0.00048, (torch.Size([512]))
2023-02-27 11:44:30,896 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_qs.bias: 0.00029, (torch.Size([512]))
2023-02-27 11:44:30,897 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_qs.bias: 0.00027, (torch.Size([512]))
2023-02-27 11:44:30,897 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.1.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2023-02-27 11:44:30,897 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_edge.layer_stack.0.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2023-02-27 11:44:30,897 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.3.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2023-02-27 11:44:30,897 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.2.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2023-02-27 11:44:30,897 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.1.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2023-02-27 11:44:30,897 maskrcnn_benchmark INFO: roi_heads.relation.predictor.context_layer.context_obj.layer_stack.0.slf_attn.w_ks.bias: 0.00000, (torch.Size([512]))
2023-02-27 11:44:30,897 maskrcnn_benchmark INFO: -------------------------------
2023-02-27 11:44:30,901 maskrcnn_benchmark INFO: eta: 10:53:04  iter: 8000  loss: 4.2655 (4.7980)  loss_rel: 0.2369 (0.7694)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2490 (0.7536)  data: 0.0028 (0.4909)  lr: 0.000250  max mem: 4656
2023-02-27 11:44:30,904 maskrcnn_benchmark.utils.checkpoint INFO: Saving checkpoint to /users/students/r0879687/amager/vg/output/relation_baseline/model_0008000.pth
2023-02-27 11:44:49,736 maskrcnn_benchmark INFO: Start validating
2023-02-27 11:44:49,744 maskrcnn_benchmark INFO: Start evaluation on VG_stanford_filtered_with_attribute_val dataset(5000 images).
2023-02-27 12:00:32,250 maskrcnn_benchmark INFO: Total run time: 0:15:42.505128 (0.18850102558135987 s / img per device, on 1 devices)
2023-02-27 12:00:32,282 maskrcnn_benchmark INFO: Model inference time: 0:15:24.145414 (0.18482908287048339 s / img per device, on 1 devices)
creating index...
index created!
Loading and preparing results...
Converting ndarray to lists...
(62754, 7)
0/62754
DONE (t=0.46s)
creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=17.24s).
Accumulating evaluation results...
DONE (t=3.22s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.675
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.994
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000
2023-02-27 12:03:27,394 maskrcnn_benchmark INFO: 
====================================================================================================
Detection evaluation mAp=0.9999
====================================================================================================
SGG eval:   R @ 20: 0.5065;   R @ 50: 0.5549;   R @ 100: 0.5717;  for mode=predcls, type=Recall(Main).
SGG eval: ngR @ 20: 0.5569; ngR @ 50: 0.6678; ngR @ 100: 0.7450;  for mode=predcls, type=No Graph Constraint Recall(Main).
SGG eval:  zR @ 20: 0.0089;  zR @ 50: 0.0556;  zR @ 100: 0.0904;  for mode=predcls, type=Zero Shot Recall.
SGG eval:  zR @ 20: 0.0650;  zR @ 50: 0.1015;  zR @ 100: 0.1170;  for mode=predcls, type=1 Shot Recall.
 zR @ 20: 0.0676;  zR @ 50: 0.0931;  zR @ 100: 0.1216;  for mode=predcls, type=2 Shot Recall.
 zR @ 20: 0.0989;  zR @ 50: 0.1374;  zR @ 100: 0.1703;  for mode=predcls, type=3 Shot Recall.
 zR @ 20: 0.1358;  zR @ 50: 0.1835;  zR @ 100: 0.2114;  for mode=predcls, type=4 Shot Recall.
 zR @ 20: 0.1305;  zR @ 50: 0.1469;  zR @ 100: 0.1623;  for mode=predcls, type=5 Shot Recall.
 zR @ 20: 0.1538;  zR @ 50: 0.2547;  zR @ 100: 0.2769;  for mode=predcls, type=6 Shot Recall.
 zR @ 20: 0.0833;  zR @ 50: 0.1361;  zR @ 100: 0.1622;  for mode=predcls, type=7 Shot Recall.
 zR @ 20: 0.1636;  zR @ 50: 0.2000;  zR @ 100: 0.2273;  for mode=predcls, type=8 Shot Recall.
 zR @ 20: 0.2050;  zR @ 50: 0.3058;  zR @ 100: 0.3273;  for mode=predcls, type=9 Shot Recall.
 zR @ 20: 0.1358;  zR @ 50: 0.1791;  zR @ 100: 0.2264;  for mode=predcls, type=10 Shot Recall.
 zR @ 20: 0.1690;  zR @ 50: 0.2329;  zR @ 100: 0.2967;  for mode=predcls, type=20 Shot Recall.
 zR @ 20: 0.1512;  zR @ 50: 0.2036;  zR @ 100: 0.2286;  for mode=predcls, type=25 Shot Recall.
 zR @ 20: 0.1169;  zR @ 50: 0.1429;  zR @ 100: 0.1429;  for mode=predcls, type=30 Shot Recall.
 zR @ 20: 0.4884;  zR @ 50: 0.5640;  zR @ 100: 0.6047;  for mode=predcls, type=40 Shot Recall.
 zR @ 20: 0.3542;  zR @ 50: 0.5000;  zR @ 100: 0.5469;  for mode=predcls, type=50 Shot Recall.
 zR @ 20: 0.3077;  zR @ 50: 0.3077;  zR @ 100: 0.3718;  for mode=predcls, type=100 Shot Recall.
 zR @ 20: 1.0000;  zR @ 50: 1.0000;  zR @ 100: 1.0000;  for mode=predcls, type=200 Shot Recall.
SGG eval:  mR @ 20: 0.0512;  mR @ 50: 0.0615;  mR @ 100: 0.0661;  for mode=predcls, type=Mean Recall.
(above:0.0029) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0000) (attached to:0.0000) (behind:0.0077) (belonging to:0.0000) (between:0.0000) (carrying:0.0000) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0000) (has:0.5765) (holding:0.5307) (in:0.0149) (in front of:0.0000) (laying on:0.0000) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.0000) (of:0.0033) (on:0.9851) (on back of:0.0000) (over:0.0000) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.0000) (says:0.0000) (sitting on:0.0000) (standing on:0.0000) (to:0.0000) (under:0.0383) (using:0.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.0000) (wearing:0.9606) (wears:0.0000) (with:0.1853) 
SGG eval:   A @ 20: 0.6139;   A @ 50: 0.6181;   A @ 100: 0.6181;  for mode=predcls, type=TopK Accuracy.
====================================================================================================

2023-02-27 12:03:27,941 maskrcnn_benchmark INFO: Validation Result: 0.5717
2023-02-27 12:04:19,973 maskrcnn_benchmark INFO: eta: 12:39:53  iter: 8200  loss: 4.3123 (4.7912)  loss_rel: 0.2837 (0.7626)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2470 (0.8802)  data: 0.0029 (0.6177)  lr: 0.000250  max mem: 4656
2023-02-27 12:05:11,332 maskrcnn_benchmark INFO: eta: 12:24:11  iter: 8400  loss: 4.2448 (4.7833)  loss_rel: 0.2162 (0.7547)  loss_refine_obj: 4.0286 (4.0286)  time: 0.2535 (0.8653)  data: 0.0028 (0.6031)  lr: 0.000250  max mem: 4656
